{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/olive/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1000e8c67be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselect_n_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/dme-sentiment/helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextblob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Find the optimal number of components to maintain from SVD,PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import Preprocessor\n",
    "from data_loader import DataLoader\n",
    "from helpers import select_n_components, pos_check\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239073/239073 [00:05<00:00, 43034.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = DataLoader().create_dataframe(preprocess=True, split=True, remove_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_clean</th>\n",
       "      <th>sentiment_val</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64633</th>\n",
       "      <td>3748</td>\n",
       "      <td>a fairy tale that comes from a renowned indian...</td>\n",
       "      <td>fairy tale comes renowned indian film culture ...</td>\n",
       "      <td>0.88889</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23483</th>\n",
       "      <td>183087</td>\n",
       "      <td>Chris Cooper 's</td>\n",
       "      <td>Chris Cooper</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93051</th>\n",
       "      <td>229624</td>\n",
       "      <td>call it a work of art</td>\n",
       "      <td>call work art</td>\n",
       "      <td>0.76389</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15507</th>\n",
       "      <td>103745</td>\n",
       "      <td>A muddled limp biscuit</td>\n",
       "      <td>A muddled limp biscuit</td>\n",
       "      <td>0.19444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84727</th>\n",
       "      <td>115229</td>\n",
       "      <td>ate</td>\n",
       "      <td>ate</td>\n",
       "      <td>0.36111</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       phrase_id                                             phrase  \\\n",
       "64633       3748  a fairy tale that comes from a renowned indian...   \n",
       "23483     183087                                    Chris Cooper 's   \n",
       "93051     229624                              call it a work of art   \n",
       "15507     103745                             A muddled limp biscuit   \n",
       "84727     115229                                                ate   \n",
       "\n",
       "                                            phrase_clean  sentiment_val  \\\n",
       "64633  fairy tale comes renowned indian film culture ...        0.88889   \n",
       "23483                                       Chris Cooper        0.50000   \n",
       "93051                                      call work art        0.76389   \n",
       "15507                             A muddled limp biscuit        0.19444   \n",
       "84727                                                ate        0.36111   \n",
       "\n",
       "       label_id          label  \n",
       "64633       5.0  Very positive  \n",
       "23483       3.0        Neutral  \n",
       "93051       4.0       Positive  \n",
       "15507       1.0  Very negative  \n",
       "84727       2.0       Negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119468, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, test, dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['phrase_clean'], train['label_id']\n",
    "X_val, y_val = validate['phrase_clean'], validate['label_id']\n",
    "X_test, y_test = test['phrase_clean'], test['label_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection\n",
    "\n",
    "- Features to include:\n",
    "\n",
    "    - phrase length\n",
    "    - punctuation count\n",
    "    - capital letters count\n",
    "    - number of adjective POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_count = lambda l1, l2: sum([1 for x in l1 if x in l2])\n",
    "caps_count = lambda l1: sum([1 for x in l1 if x.isupper()])\n",
    "\n",
    "def get_phrase_length(text):\n",
    "    return np.array([len(t) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_punct(text):\n",
    "    return np.array([punct_count(t, set(string.punctuation)) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_caps(text):\n",
    "    return np.array([caps_count(t) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_adj_pos(text):\n",
    "    return np.array([pos_check(t) for t in tqdm(text)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', \n",
    "                             max_features=10000, \n",
    "                             use_idf=True, \n",
    "                             ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_pipeline(feature_name, feature_id, feature):\n",
    "    return (feature_name, Pipeline([\n",
    "        (feature_id, feature)\n",
    "    ]))\n",
    "\n",
    "def create_pipeline(my_id, clf, vectorizer=tfidf_vect, use_features=True):\n",
    "    if use_features:\n",
    "        pipe = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    ('chi2', SelectKBest(chi2, k=1000))\n",
    "                ])),\n",
    "                create_feature_pipeline('phrase_length', 'f1', FunctionTransformer(get_phrase_length, validate=False)),\n",
    "                create_feature_pipeline('num_punct', 'f2', FunctionTransformer(get_num_punct, validate=False)),\n",
    "                create_feature_pipeline('num_caps', 'f3', FunctionTransformer(get_num_caps, validate=False)),\n",
    "                create_feature_pipeline('num_adj_pos', 'f4', FunctionTransformer(get_num_adj_pos, validate=False))\n",
    "            ])),\n",
    "            (my_id, clf)            \n",
    "        ])\n",
    "    else:\n",
    "        pipe = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    ('chi2', SelectKBest(chi2, k=1000))\n",
    "                ])),\n",
    "            ])),\n",
    "            (my_id, clf)            \n",
    "        ])\n",
    "        \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classifiers with TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Dummy, most frequent': create_pipeline(my_id='dc', \n",
    "                                            clf=DummyClassifier(strategy='most_frequent'), \n",
    "                                            use_features=True),\n",
    "    'Logistic Regression': create_pipeline(my_id='lr', \n",
    "                                        clf=LogisticRegression(max_iter=5000), \n",
    "                                        use_features=True),\n",
    "    'kNN': create_pipeline(my_id='knn', clf=KNeighborsClassifier(n_neighbors=5), \n",
    "                           use_features=True),\n",
    "    'Linear SVM': create_pipeline(my_id='svm', \n",
    "                               clf=SVC(kernel='linear'), \n",
    "                               use_features=True),\n",
    "    'RBF SVM': create_pipeline(my_id='svm_rbf', \n",
    "                               clf=SVC(kernel='rbf'), \n",
    "                               use_features=True),\n",
    "    'Random Forest': create_pipeline(my_id='rf', \n",
    "                                     clf=RandomForestClassifier(max_depth=10, n_estimators=50),\n",
    "                                     use_features=True),\n",
    "    'MLP Classifer': create_pipeline(my_id='mlp',\n",
    "                                    clf=MLPClassifier(max_iter=800),\n",
    "                                    use_features=True),\n",
    "    'MLP Classifer (stronger)': create_pipeline(my_id='mlp',\n",
    "                                    clf=MLPClassifier(max_iter=800, alpha=1),\n",
    "                                    use_features=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119468/119468 [00:00<00:00, 2246857.22it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 394393.10it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 439075.37it/s]\n",
      "100%|██████████| 119468/119468 [01:09<00:00, 1716.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training classifier: Dummy, most frequent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119468/119468 [00:00<00:00, 2285774.09it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 390454.51it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 451212.58it/s]\n",
      "100%|██████████| 119468/119468 [01:05<00:00, 1829.90it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 2251131.89it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 399358.17it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 433689.06it/s]\n",
      "100%|██████████| 47470/47470 [00:25<00:00, 1886.81it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 2270116.61it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 396509.97it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 450735.28it/s]\n",
      "100%|██████████| 119468/119468 [01:02<00:00, 1915.23it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 2213984.33it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 392553.66it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 348209.25it/s]\n",
      "100%|██████████| 47470/47470 [00:22<00:00, 2148.57it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 2282910.29it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 390049.97it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 426010.55it/s]\n",
      "100%|██████████| 119468/119468 [01:02<00:00, 1902.56it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training classifier: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119468/119468 [00:00<00:00, 2308850.06it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 398093.22it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 441640.71it/s]\n",
      "100%|██████████| 119468/119468 [01:03<00:00, 1874.23it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 2175662.86it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 395829.08it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 435537.11it/s]\n",
      "100%|██████████| 47470/47470 [00:25<00:00, 1896.21it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 2293621.11it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 392663.00it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 440233.41it/s]\n",
      "100%|██████████| 119468/119468 [01:03<00:00, 1881.85it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 2284713.14it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 398155.46it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 478119.09it/s]\n",
      "100%|██████████| 47470/47470 [00:25<00:00, 1888.49it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 2292949.40it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 396473.58it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 427092.96it/s]\n",
      "100%|██████████| 119468/119468 [01:03<00:00, 1875.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training classifier: kNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119468/119468 [00:00<00:00, 2259012.47it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 396556.41it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 453325.05it/s]\n",
      "100%|██████████| 119468/119468 [01:03<00:00, 1873.86it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 2244280.75it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 396364.13it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 453884.30it/s]\n",
      "100%|██████████| 47470/47470 [00:22<00:00, 2149.71it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 2288007.63it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 395046.99it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 442611.29it/s]\n",
      "100%|██████████| 119468/119468 [01:02<00:00, 1900.81it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 2093205.47it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 394441.10it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 456911.17it/s]\n",
      "100%|██████████| 47470/47470 [00:25<00:00, 1893.07it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 2069316.99it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 392133.55it/s]\n",
      "100%|██████████| 119468/119468 [00:00<00:00, 446715.78it/s]\n",
      "100%|██████████| 119468/119468 [01:00<00:00, 1977.01it/s]\n"
     ]
    }
   ],
   "source": [
    "ca_train_score = {}\n",
    "ca_val_score = {}\n",
    "\n",
    "ce_train_score = {} \n",
    "ce_val_score = {} \n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'\\nFinished training classifier: {name}')\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    \n",
    "    ca_train_score[name] = f1_score(train_preds, y_train, average='micro')\n",
    "    ca_val_score[name] = f1_score(val_preds, y_val, average='micro')\n",
    "    \n",
    "    ce_train_score[name] = log_loss(y_train, clf.predict_proba(X_train))\n",
    "    ce_val_score[name] = log_loss(y_val, clf.predict_proba(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification performance on validation set: \\n')\n",
    "\n",
    "print('Validation (LHS), Training (RHS)')\n",
    "print()\n",
    "for name, clf in classifiers.items():\n",
    "    print (\"{method:<20s}{val_f1:>13.3f}{val_logloss:>13.3f}{train_f1:>13.3f}{train_logloss:>13.3f}\".format(\n",
    "        method=clf, val_accuracy=ca_val_score[clf], val_logloss=ce_val_score[clf],\n",
    "        train_accuracy=ca_train_score[clf], train_logloss=ce_train_score[clf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Dummy, most frequent': create_pipeline(my_id='dc', \n",
    "                                            clf=DummyClassifier(strategy='most_frequent'), \n",
    "                                            use_features=False),\n",
    "    'Logistic Regression': create_pipeline(my_id='lr', \n",
    "                                        clf=LogisticRegression(max_iter=5000), \n",
    "                                        use_features=False),\n",
    "    'kNN': create_pipeline(my_id='knn', clf=KNeighborsClassifier(n_neighbors=5), \n",
    "                           use_features=False),\n",
    "    'Linear SVM': create_pipeline(my_id='svm', \n",
    "                               clf=SVC(kernel='linear'), \n",
    "                               use_features=False),\n",
    "    'RBF SVM': create_pipeline(my_id='svm_rbf', \n",
    "                               clf=SVC(kernel='rbf'), \n",
    "                               use_features=False),\n",
    "    'Random Forest': create_pipeline(my_id='rf', \n",
    "                                     clf=RandomForestClassifier(max_depth=10, n_estimators=50),\n",
    "                                     use_features=False),\n",
    "    'MLP Classifer': create_pipeline(my_id='mlp',\n",
    "                                    clf=MLPClassifier(max_iter=800),\n",
    "                                    use_features=False),\n",
    "    'MLP Classifer (stronger)': create_pipeline(my_id='mlp',\n",
    "                                    clf=MLPClassifier(max_iter=800, alpha=1),\n",
    "                                    use_features=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification performance on validation set: \\n')\n",
    "\n",
    "print('Validation (LHS), Training (RHS)')\n",
    "print()\n",
    "for name, clf in classifiers.items():\n",
    "    print (\"{method:<20s}{val_f1:>13.3f}{val_logloss:>13.3f}{train_f1:>13.3f}{train_logloss:>13.3f}\".format(\n",
    "        method=clf, val_accuracy=ca_val_score[clf], val_logloss=ce_val_score[clf],\n",
    "        train_accuracy=ca_train_score[clf], train_logloss=ce_train_score[clf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering\n",
    "- Ignore this for now.\n",
    "\n",
    "- KMeans does not work well on high-dimensional data, better to reduce dimensionality of data first, and then do KMeans on reduced space: https://stats.stackexchange.com/questions/199501/user-segmentation-by-clustering-with-sparse-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases, labels = phrase_df['phrase_clean'], phrase_df['label_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_labels = phrase_df['label_id'].values.tolist()\n",
    "true_k = np.unique(k_labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                             max_features=100,\n",
    "                             use_idf=True, \n",
    "                             ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(phrase_df['phrase_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=500)\n",
    "# normalizer = Normalizer(copy=False)\n",
    "# lsa = make_pipeline(svd, normalizer)\n",
    "# X = lsa.fit_transform(X_vect)\n",
    "\n",
    "# print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method for finding optimal number of clusters\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=1, max_iter=100, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=6, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(X)\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "# order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"Cluster %d: \" % i, end='')\n",
    "    for idx in order_centroids[i, :200]:\n",
    "        print(' %s' % terms[idx], end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
