{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gtvfv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn\n",
    "\n",
    "from preprocessing import Preprocessor\n",
    "from data_loader import DataLoader\n",
    "from helpers import select_n_components, pos_check\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 239073/239073 [00:17<00:00, 13555.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = DataLoader().create_dataframe(preprocess=True, split=True, remove_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_clean</th>\n",
       "      <th>sentiment_val</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64633</th>\n",
       "      <td>3748</td>\n",
       "      <td>a fairy tale that comes from a renowned indian...</td>\n",
       "      <td>fairy tale comes renowned indian film culture ...</td>\n",
       "      <td>0.88889</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23483</th>\n",
       "      <td>183087</td>\n",
       "      <td>Chris Cooper 's</td>\n",
       "      <td>Chris Cooper</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93051</th>\n",
       "      <td>229624</td>\n",
       "      <td>call it a work of art</td>\n",
       "      <td>call work art</td>\n",
       "      <td>0.76389</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15507</th>\n",
       "      <td>103745</td>\n",
       "      <td>A muddled limp biscuit</td>\n",
       "      <td>A muddled limp biscuit</td>\n",
       "      <td>0.19444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84727</th>\n",
       "      <td>115229</td>\n",
       "      <td>ate</td>\n",
       "      <td>ate</td>\n",
       "      <td>0.36111</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       phrase_id                                             phrase  \\\n",
       "64633       3748  a fairy tale that comes from a renowned indian...   \n",
       "23483     183087                                    Chris Cooper 's   \n",
       "93051     229624                              call it a work of art   \n",
       "15507     103745                             A muddled limp biscuit   \n",
       "84727     115229                                                ate   \n",
       "\n",
       "                                            phrase_clean  sentiment_val  \\\n",
       "64633  fairy tale comes renowned indian film culture ...        0.88889   \n",
       "23483                                       Chris Cooper        0.50000   \n",
       "93051                                      call work art        0.76389   \n",
       "15507                             A muddled limp biscuit        0.19444   \n",
       "84727                                                ate        0.36111   \n",
       "\n",
       "       label_id          label  \n",
       "64633       5.0  Very positive  \n",
       "23483       3.0        Neutral  \n",
       "93051       4.0       Positive  \n",
       "15507       1.0  Very negative  \n",
       "84727       2.0       Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, test, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['phrase_clean'], train['label_id']\n",
    "X_val, y_val = validate['phrase_clean'], validate['label_id']\n",
    "X_test, y_test = test['phrase_clean'], test['label_id']\n",
    "\n",
    "datasets = [X_train, X_val, X_test]\n",
    "labels = [y_train, y_val, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming text to tf-idf vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_length = 2500\n",
    "vec_length = tfidf_vec_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_datasets = []\n",
    "\n",
    "for d in datasets:\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word', max_features=tfidf_vec_length, use_idf=True, ngram_range=(1,3))\n",
    "    tf_datasets.append(tfidf_vec.fit_transform(d).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset_arrays = []\n",
    "for d in tf_datasets:\n",
    "    tf_dataset_arrays.append(np.asarray(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train or load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"started\")\n",
    "lr_init = 1e-3\n",
    "l2_penalty = 1e-2\n",
    "embed_size = 2\n",
    "batch_size = 'auto'\n",
    "n_iter_no_change = 3\n",
    "tol = 1e-6\n",
    "max_iter = 25\n",
    "\n",
    "nnets = []\n",
    "sizes = []\n",
    "\n",
    "while embed_size < vec_length:\n",
    "    log_vec_l, log_embed_l = math.log2(vec_length), math.log2(embed_size)\n",
    "    log_hid_l = (log_vec_l + log_embed_l) / 2\n",
    "    hid_l = int(math.pow(2, log_hid_l))\n",
    "    hid = (hid_l, embed_size, hid_l)\n",
    "    \n",
    "    # auto-encoder\n",
    "    regressor = MLPRegressor(hidden_layer_sizes=hid, activation='relu', learning_rate='adaptive', random_state=420, \n",
    "                             verbose=True, batch_size=batch_size, alpha=l2_penalty, n_iter_no_change=n_iter_no_change, \n",
    "                             learning_rate_init=lr_init, tol=tol, max_iter=max_iter)\n",
    "    filename = 'autoencoders/autoencoder' + str(hid) + '.sav'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        nnet = pickle.load(open(filename, 'rb'))\n",
    "        print(\"Loaded \" + str(hid))\n",
    "    else:\n",
    "        # Fit only on datasets[0], which is training data\n",
    "        print(\"Fitting \" + str(hid))\n",
    "        nnet = regressor.fit(tf_datasets[0], tf_datasets[0])\n",
    "        pickle.dump(nnet, open(filename, 'wb'))\n",
    "        print(\"Done fitting.\")\n",
    "    \n",
    "    nnets.append(nnet)\n",
    "    sizes.append(hid)\n",
    "    embed_size *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_nnet = []\n",
    "embedding_layer_num = 3\n",
    "\n",
    "for d in tf_dataset_arrays:\n",
    "    embeddings_dataset = []\n",
    "    for i, n in enumerate(sizes):\n",
    "        embed_size = n[1]\n",
    "\n",
    "        nnet = nnets[i]\n",
    "\n",
    "        hidden_layer_sizes = list(n)\n",
    "        layer_units = [tfidf_vec_length] + hidden_layer_sizes + [tfidf_vec_length]\n",
    "\n",
    "        print(\"Computing embeddings for nnet \" + str(layer_units))\n",
    "        activations = [d]\n",
    "        for i in range(nnet.n_layers_ - 1):\n",
    "            activations.append(np.empty((d.shape[0], layer_units[i + 1])))\n",
    "        nnet._forward_pass(activations)\n",
    "        embeddings_dataset.append(activations[embedding_layer_num - 1])\n",
    "    embeddings_nnet.append(embeddings_dataset)\n",
    "embeddings.append(embeddings_nnet)\n",
    "print(\"Embeddings computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_train_size = 20000\n",
    "scs_dataset_size = 10000\n",
    "acs_dataset_size = 20000\n",
    "supervised_dp_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_umap = []\n",
    "\n",
    "d = np.concatenate([tf_dataset_arrays[0][0:umap_train_size], tf_dataset_arrays[1][0:supervised_dp_size], tf_dataset_arrays[2][0:acs_dataset_size]],\n",
    "                   axis=0) \n",
    "print(d.shape)\n",
    "\n",
    "embeddings_dataset_train = []\n",
    "embeddings_dataset_val = []\n",
    "embeddings_dataset_test = []\n",
    "\n",
    "\n",
    "for j, n in enumerate(sizes): \n",
    "    embed_size = n[1] # same embedding sizes as in neural net\n",
    "\n",
    "    umap_reducer = umap.UMAP(n_components=embed_size, random_state=420)\n",
    "\n",
    "    print(\"Computing embeddings for umap \" + str(embed_size))\n",
    "\n",
    "    embedding = umap_reducer.fit_transform(d)\n",
    "    \n",
    "    embeddings_dataset_train.append(embedding[0:umap_train_size])\n",
    "    embeddings_dataset_val.append(embedding[umap_train_size:umap_train_size+supervised_dp_size])\n",
    "    embeddings_dataset_test.append(embedding[umap_train_size+supervised_dp_size::])\n",
    "        \n",
    "    \n",
    "embeddings.append([embeddings_dataset_train, embeddings_dataset_val, embeddings_dataset_test])\n",
    "\n",
    "print(\"Embeddings computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(embeddings):\n",
    "    for j, d in enumerate(e):\n",
    "        d.append(tf_dataset_arrays[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save embeddings. Later will be loaded separately to save memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(embeddings):\n",
    "    for j, d in enumerate(embeddings[i]):\n",
    "        for k, s in enumerate(embeddings[i][j]):\n",
    "            name = \"embeddings/embeddings_\" + str(i) + \"_\" + str(j) + \"_\" + str(k)\n",
    "            np.save(name, embeddings[i][j][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize and train models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans with 5 clusters and 2 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 2 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 2 sized embeddings.\n",
      "Kmeans with 5 clusters and 4 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 4 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 4 sized embeddings.\n",
      "Kmeans with 5 clusters and 8 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 8 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 8 sized embeddings.\n",
      "Kmeans with 5 clusters and 16 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 16 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 16 sized embeddings.\n",
      "Kmeans with 5 clusters and 32 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 32 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 32 sized embeddings.\n",
      "Kmeans with 5 clusters and 64 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 64 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 64 sized embeddings.\n",
      "Kmeans with 5 clusters and 128 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 128 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 128 sized embeddings.\n",
      "Kmeans with 5 clusters and 256 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 256 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 256 sized embeddings.\n",
      "Kmeans with 5 clusters and 512 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 512 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 512 sized embeddings.\n",
      "Kmeans with 5 clusters and 1024 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 1024 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 1024 sized embeddings.\n",
      "Kmeans with 5 clusters and 2048 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 2048 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 2048 sized embeddings.\n",
      "Kmeans with 5 clusters and 2500 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 2500 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 2500 sized embeddings.\n",
      "Kmeans with 5 clusters and 2 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 2 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 2 sized embeddings.\n",
      "Kmeans with 5 clusters and 4 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 4 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 4 sized embeddings.\n",
      "Kmeans with 5 clusters and 8 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 8 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 8 sized embeddings.\n",
      "Kmeans with 5 clusters and 16 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 16 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 16 sized embeddings.\n",
      "Kmeans with 5 clusters and 32 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 32 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 32 sized embeddings.\n",
      "Kmeans with 5 clusters and 64 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 64 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 64 sized embeddings.\n",
      "Kmeans with 5 clusters and 128 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 128 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 128 sized embeddings.\n",
      "Kmeans with 5 clusters and 256 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 256 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 256 sized embeddings.\n",
      "Kmeans with 5 clusters and 512 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 512 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 512 sized embeddings.\n",
      "Kmeans with 5 clusters and 1024 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 1024 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 1024 sized embeddings.\n",
      "Kmeans with 5 clusters and 2048 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 2048 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 2048 sized embeddings.\n",
      "Kmeans with 5 clusters and 2500 sized embeddings.\n",
      "SpectralClustering with 5 clusters and 2500 sized embeddings.\n",
      "AgglomerativeClustering with 5 clusters and 2500 sized embeddings.\n"
     ]
    }
   ],
   "source": [
    "k_means = []\n",
    "scs = []\n",
    "acs = []\n",
    "\n",
    "clusters = 5\n",
    "\n",
    "i_ = 2\n",
    "j_ = 12\n",
    "\n",
    "for i in range(i_):\n",
    "    k_means_e = []\n",
    "    scs_e = []\n",
    "    acs_e = []\n",
    "    for j in range(j_): #(embeddings[i][0]):\n",
    "        d = np.load(\"embeddings/embeddings_\" + str(i) + \"_\" + str(0) + \"_\" + str(j) + \".npy\")\n",
    "        print(\"Kmeans with \" + str(clusters) + \" clusters and \" + str(d.shape[1]) + \" sized embeddings.\")\n",
    "        kmeans = KMeans(n_clusters=clusters, init='k-means++', n_init=1, max_iter=100, random_state=420)\n",
    "        k_means_e.append(kmeans.fit(d))\n",
    "\n",
    "        print(\"SpectralClustering with \" + str(clusters) + \" clusters and \" + str(d.shape[1]) + \" sized embeddings.\")\n",
    "        sc = SpectralClustering(n_clusters=clusters)\n",
    "        #scs.append(sc.fit(d[::scs_dataset_size]))\n",
    "        scs_e.append(sc)\n",
    "\n",
    "        print(\"AgglomerativeClustering with \" + str(clusters) + \" clusters and \" + str(d.shape[1]) + \" sized embeddings.\")\n",
    "        ac = AgglomerativeClustering(n_clusters=clusters)\n",
    "        #acs.append(ac.fit(d[::acs_dataset_size]))\n",
    "        acs_e.append(ac)\n",
    "    k_means.append(k_means_e)\n",
    "    scs.append(scs_e)\n",
    "    acs.append(acs_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create mappings from clusters to labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(pred, gt, clusters):\n",
    "    labels = list(range(1, clusters+1))\n",
    "    argmax_map = labels\n",
    "    max_map = 0\n",
    "    for p in permutations(labels, clusters):\n",
    "        match_count = 0\n",
    "        for i in range(pred.shape[0]):\n",
    "            if p.index(pred[i]+1) == labels.index(int(gt[i])):\n",
    "                match_count+=1\n",
    "        if match_count > max_map:\n",
    "            argmax_map = p\n",
    "            max_map = match_count\n",
    "    return argmax_map\n",
    "\n",
    "def map_labels(pred, label_map):\n",
    "    for i in range(pred.shape[0]):\n",
    "        pred[i] = label_map.index(pred[i]+1)+1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(y_val, preds):\n",
    "    preds = preds.astype(int)\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    precision = precision_score(y_val, preds, average='macro')\n",
    "    recall = recall_score(y_val, preds, average='macro')\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    kappa = cohen_kappa_score(y_val, preds)\n",
    "    return [accuracy, precision, recall, f1, kappa]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute all predictions. Printed predictions were just manually inserted to Plotting.ipynb for convenience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.49195281230250687, 0.20685306076636972, 0.2014907588343128, 0.13715645877987015, 0.0022928586756458813]\n",
      "[0.4876, 0.3111650940239577, 0.20040166293250244, 0.13311205585266767, 0.0010190814128702153]\n",
      "(20200, 2)\n",
      "[0.4918, 0.19944346999224394, 0.20120781607257104, 0.1386580616086914, 0.005086884845154849]\n",
      "0\n",
      "[0.4068, 0.19971716241163373, 0.19802040755999667, 0.17906826419238744, -5.149034498286298e-05]\n",
      "(20200, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-07c3aa67ac05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mac_pred_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_val_sub_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0macs_dataset_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac_pred_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mac_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac_pred_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mac_pred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mac_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msupervised_dp_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mac_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac_pred_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_sub_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    923\u001b[0m             \u001b[0mCluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m         \"\"\"\n\u001b[1;32m--> 925\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    880\u001b[0m                                          \u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m                                          \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_distance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m                                          **kwargs)\n\u001b[0m\u001b[0;32m    883\u001b[0m         (self.children_,\n\u001b[0;32m    884\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_connected_components_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\u001b[0m in \u001b[0;36mward_tree\u001b[1;34m(X, connectivity, n_clusters, return_distance)\u001b[0m\n\u001b[0;32m    234\u001b[0m                           stacklevel=2)\n\u001b[0;32m    235\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequirements\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[0mchildren_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\cluster\\hierarchy.py\u001b[0m in \u001b[0;36mward\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m     \"\"\"\n\u001b[1;32m--> 830\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gtvfv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\cluster\\hierarchy.py\u001b[0m in \u001b[0;36mlinkage\u001b[1;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_hierarchy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmst_single_linkage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'complete'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'average'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_hierarchy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_hierarchy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_linkage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_hierarchy.pyx\u001b[0m in \u001b[0;36mscipy.cluster._hierarchy.nn_chain\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margsort\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_val_sub_set = labels[1][0:supervised_dp_size].to_numpy().astype(int)\n",
    "y_test_set = labels[2]\n",
    "\n",
    "k_means_metrics = []\n",
    "sc_metrics = []\n",
    "ac_metrics = []\n",
    "kmeans_map = []\n",
    "\n",
    "i_ = 2\n",
    "j_ = 12\n",
    "\n",
    "# Validation set\n",
    "for i in range(i_):\n",
    "    for j in range(j_):\n",
    "        print(j)\n",
    "        x_train_set = np.load(\"embeddings/embeddings_\" + str(i) + \"_\" + str(0) + \"_\" + str(j) + \".npy\") #embeddings[i][0][j]\n",
    "        x_test_set = np.load(\"embeddings/embeddings_\" + str(i) + \"_\" + str(2) + \"_\" + str(j) + \".npy\")\n",
    "        x_val_sub_set = np.load(\"embeddings/embeddings_\" + str(i) + \"_\" + str(1) + \"_\" + str(j) + \".npy\")[0:supervised_dp_size]\n",
    "\n",
    "        # kmeans predictions on val and test set\n",
    "        k_means_pred_set = np.concatenate([x_val_sub_set, x_test_set], axis=0)\n",
    "        k_means_pred = k_means[i][j].predict(k_means_pred_set)\n",
    "        k_means_pred_val = k_means_pred[0:supervised_dp_size]\n",
    "        k_means_map = label_map(k_means_pred_val, y_val_sub_set, clusters)\n",
    "        kmeans_map.append(k_means_map) \n",
    "        k_means_pred_test = map_labels(k_means_pred[supervised_dp_size::], k_means_map)\n",
    "        k_means_metric = test_metrics(y_test_set[0:len(k_means_pred_test)].to_numpy().astype(int), k_means_pred_test)\n",
    "        k_means_metrics.append(k_means_metrics)\n",
    "        print(k_means_metric)\n",
    "        \n",
    "        # don't compute spectral clustering with UMAP embeddings due to computational constraints\n",
    "        if i == 0:\n",
    "            # sc and ac computations and predictions on val and test sets\n",
    "            sc_pred_set = np.concatenate([x_val_sub_set, x_test_set[0:scs_dataset_size]], axis=0)\n",
    "            sc_pred = scs[i][j].fit_predict(sc_pred_set)\n",
    "            sc_pred_val = sc_pred[0:supervised_dp_size]\n",
    "            sc_map = label_map(sc_pred_val, y_val_sub_set, clusters)\n",
    "            sc_pred_test = map_labels(sc_pred[supervised_dp_size:scs_dataset_size + supervised_dp_size], sc_map)\n",
    "            sc_metric = test_metrics(y_test_set[0:scs_dataset_size].to_numpy().astype(int), sc_pred_test)\n",
    "            sc_metrics.append(sc_metrics)\n",
    "            print(sc_metric)\n",
    "\n",
    "        ac_pred_set = np.concatenate([x_val_sub_set, x_test_set[0:acs_dataset_size]], axis=0)\n",
    "        print(ac_pred_set.shape)\n",
    "        ac_pred = acs[i][j].fit_predict(ac_pred_set)\n",
    "        ac_pred_val = ac_pred[0:supervised_dp_size]\n",
    "        ac_map = label_map(ac_pred_val, y_val_sub_set, clusters)\n",
    "        ac_pred_test = map_labels(ac_pred[supervised_dp_size:acs_dataset_size + supervised_dp_size], ac_map)\n",
    "        ac_metric = test_metrics(y_test_set[0:acs_dataset_size].to_numpy().astype(int), ac_pred_test)\n",
    "        ac_metrics.append(ac_metric)\n",
    "        print(ac_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**! Metrics plotted in Plotting.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    fig, axs = plt.subplots()\n",
    "\n",
    "    nnet = np.load(\"embeddings/embeddings_\" + str(i) + \"_\" + str(0) + \"_\" + str(0) + \".npy\")\n",
    "    centers = k_means[0][0].cluster_centers_\n",
    "\n",
    "    df_x = pd.DataFrame(nnet)\n",
    "    df_y = pd.DataFrame(np.expand_dims(np.array(labels[0], dtype='int'), axis=1))\n",
    "    df = pd.concat([df_x, df_y], axis=1)\n",
    "    df.columns = [\"1st component\", \"2nd component\", \"Sent.\"]\n",
    "    seaborn.scatterplot(data=df, x=df.columns[0], y=df.columns[1], hue=df.columns[2], \n",
    "                        palette=seaborn.diverging_palette(20, 220, as_cmap=True), alpha=0.05, legend=False, ax=axs)\n",
    "    df_x = pd.DataFrame(centers)\n",
    "    df_y = pd.DataFrame(np.expand_dims(np.array(kmeans_map[0], dtype='int'), axis=1))\n",
    "    df = pd.concat([df_x, df_y], axis=1)\n",
    "    df.columns = [\"1st component\", \"2nd component\", \"Sent.\"]\n",
    "    seaborn.scatterplot(data=df, x=df.columns[0], y=df.columns[1], hue=df.columns[2], \n",
    "                        palette=seaborn.diverging_palette(20, 220, as_cmap=True), marker='X', s=75, edgecolor='black', ax=axs)\n",
    "    fig.tight_layout() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
