{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "driven-divide",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supported-drinking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s21/s2125219/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from preprocessing import Preprocessor\n",
    "from data_loader import DataLoader, Encoder\n",
    "from helpers import select_n_components, pos_check\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-transcript",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corresponding-mirror",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3542/239073 [00:00<00:06, 35418.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239073/239073 [00:05<00:00, 43132.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = DataLoader().create_dataframe(preprocess=True, split=True, remove_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compatible-fitness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_clean</th>\n",
       "      <th>sentiment_val</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211997</th>\n",
       "      <td>38678</td>\n",
       "      <td>the story the film portrays</td>\n",
       "      <td>story film portrays</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215202</th>\n",
       "      <td>237482</td>\n",
       "      <td>think of this one</td>\n",
       "      <td>think one</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201915</th>\n",
       "      <td>95022</td>\n",
       "      <td>the Duke something of a theatrical air</td>\n",
       "      <td>Duke something theatrical air</td>\n",
       "      <td>0.56944</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212237</th>\n",
       "      <td>136377</td>\n",
       "      <td>the sweetness</td>\n",
       "      <td>sweetness</td>\n",
       "      <td>0.63889</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178664</th>\n",
       "      <td>36267</td>\n",
       "      <td>question your own firmly held positions</td>\n",
       "      <td>question firmly held positions</td>\n",
       "      <td>0.44444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        phrase_id                                   phrase  \\\n",
       "211997      38678              the story the film portrays   \n",
       "215202     237482                        think of this one   \n",
       "201915      95022   the Duke something of a theatrical air   \n",
       "212237     136377                            the sweetness   \n",
       "178664      36267  question your own firmly held positions   \n",
       "\n",
       "                          phrase_clean  sentiment_val  label_id     label  \\\n",
       "211997             story film portrays        0.50000       3.0   Neutral   \n",
       "215202                       think one        0.50000       3.0   Neutral   \n",
       "201915   Duke something theatrical air        0.56944       3.0   Neutral   \n",
       "212237                       sweetness        0.63889       4.0  Positive   \n",
       "178664  question firmly held positions        0.44444       3.0   Neutral   \n",
       "\n",
       "        word_count  \n",
       "211997           3  \n",
       "215202           2  \n",
       "201915           4  \n",
       "212237           1  \n",
       "178664           4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suburban-multimedia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126124, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-barbados",
   "metadata": {},
   "source": [
    "# Train, test, dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "still-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['phrase_clean'], train['label_id']\n",
    "X_val, y_val = validate['phrase_clean'], validate['label_id']\n",
    "X_test, y_test = test['phrase_clean'], test['label_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-valve",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection\n",
    "\n",
    "- Features to include:\n",
    "\n",
    "    - phrase length\n",
    "    - punctuation count\n",
    "    - capital letters count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "innovative-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_count = lambda l1, l2: sum([1 for x in l1 if x in l2])\n",
    "caps_count = lambda l1: sum([1 for x in l1 if x.isupper()])\n",
    "\n",
    "def get_phrase_length(text):\n",
    "    return np.array([len(t) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_punct(text):\n",
    "    return np.array([punct_count(t, set(string.punctuation)) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_caps(text):\n",
    "    return np.array([caps_count(t) for t in tqdm(text)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "twenty-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', \n",
    "                             max_features=10000, \n",
    "                             use_idf=True, \n",
    "                             ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-guitar",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faced-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.accuracy = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "        self.f1 = 0\n",
    "        self.kappa = 0\n",
    "        \n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        print('Making dense transformation...\\n')\n",
    "        return X.todense()\n",
    "\n",
    "class ClassificationPipeline():\n",
    "    def __init__(self, clf_id, clf, vectorizer, feature_processing, pipe=None):\n",
    "        self.pipe = pipe \n",
    "        self.clf_id = clf_id \n",
    "        self.clf = clf\n",
    "        self.vectorizer = vectorizer\n",
    "        self.feature_processing = feature_processing\n",
    "                   \n",
    "    def create_feature_pipeline(self, memory):\n",
    "        self.pipe = Pipeline([\n",
    "            ('feature_pipeline', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', self.vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4)),\n",
    "                ])),\n",
    "                ('feature_processing', self.feature_processing)\n",
    "            ])),\n",
    "            (self.clf_id, self.clf)\n",
    "        ], memory=memory)\n",
    "    \n",
    "    def create_pipeline(self, memory):\n",
    "        self.pipe = Pipeline([\n",
    "            ('feature_pipeline', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', self.vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4)),\n",
    "                ])),\n",
    "            ])),\n",
    "            (self.clf_id, self.clf)\n",
    "        ], memory=memory)\n",
    "            \n",
    "    def train_and_evaluate(self, X_train, y_train, X_val, y_val, memory, use_features=True):\n",
    "        if use_features:\n",
    "            self.create_feature_pipeline(memory=memory)\n",
    "        else:\n",
    "            self.create_pipeline(memory=memory)\n",
    "            \n",
    "        self.pipe.fit(X_train, y_train)\n",
    "        preds = self.pipe.predict(X_val)\n",
    "        \n",
    "        accuracy = accuracy_score(y_val, preds)\n",
    "        precision = precision_score(y_val, preds, average='macro')\n",
    "        recall = recall_score(y_val, preds, average='macro')\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        kappa = cohen_kappa_score(y_val, preds)\n",
    "        \n",
    "        return accuracy, precision, recall, f1, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alpine-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "features = FeatureUnion([\n",
    "    ('phrase_length', Pipeline([\n",
    "        ('f1', FunctionTransformer(get_phrase_length, validate=False))]\n",
    "    ))\n",
    "])\n",
    "\n",
    "feature_processing = Pipeline([('features', features)])\n",
    "\n",
    "# Classifiers\n",
    "dc = ClassificationPipeline(clf_id='dc', \n",
    "                            clf=DummyClassifier(strategy='most_frequent'),\n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "gnb = ClassificationPipeline(clf_id='gnb',\n",
    "                            clf=GaussianNB(),\n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "lr = ClassificationPipeline(clf_id='lr', \n",
    "                           clf=LogisticRegression(max_iter=10000),\n",
    "                           vectorizer=tfidf_vect,\n",
    "                           feature_processing=feature_processing)\n",
    "lin_svm = ClassificationPipeline(clf_id='lin_svm', \n",
    "                                 clf=LinearSVC(),\n",
    "                                 vectorizer=tfidf_vect,\n",
    "                                 feature_processing=feature_processing)\n",
    "rbf_svm = ClassificationPipeline(clf_id='rbf_svm', \n",
    "                                 clf=SVC(kernel='rbf'), \n",
    "                                 vectorizer=tfidf_vect,\n",
    "                                 feature_processing=feature_processing)\n",
    "rf = ClassificationPipeline(clf_id='rf', \n",
    "                            clf=RandomForestClassifier(max_depth=10, n_estimators=50), \n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "mlp = ClassificationPipeline(clf_id='mlp', \n",
    "                             clf=MLPClassifier(max_iter=800),\n",
    "                             vectorizer=tfidf_vect,\n",
    "                             feature_processing=feature_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "induced-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate classifiers using additional features\n",
    "clfs = [dc, gnb, lr, lin_svm, rbf_svm, rf, mlp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-hands",
   "metadata": {},
   "source": [
    "# Train classifiers using additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "concerned-protection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmp2msbi13o'\", use \"location='/tmp/tmp2msbi13o'\" instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dc...\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7f3be27c4190>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                               ('feature_processing',\n",
      "                                Pipeline(steps=[('features',\n",
      "                                                 FeatureUnion(transformer_list=[('phrase_length',\n",
      "                                                                                 Pipeline(steps=[('f1',\n",
      "                                                                                                  FunctionTransformer(func=<function get_phrase_length at 0x7f3be20a3dd0>))]))]))]))]), \n",
      "148695                              learning cultural clash\n",
      "166674                     political prisoners persecutions\n",
      "135059                                             intrigue\n",
      "206208                                                 foot\n",
      "178384    put gentle laughs equally gentle sentiments bu...\n",
      "                                ...                        \n",
      "107618                                        emerges movie\n",
      "166523    better films especially Seven director William...\n",
      "208626                                               mother\n",
      "219939                                               edited\n",
      "224275    try balance pointed often incisive satire unab...\n",
      "Name: phrase_clean, Length: 126124, dtype: object, \n",
      "148695    3.0\n",
      "166674    3.0\n",
      "135059    3.0\n",
      "206208    3.0\n",
      "178384    5.0\n",
      "         ... \n",
      "107618    3.0\n",
      "166523    2.0\n",
      "208626    3.0\n",
      "219939    3.0\n",
      "224275    5.0\n",
      "Name: label_id, Length: 126124, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1941015.41it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.65s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 272.8s, 4.5min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1731847.77it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gnb...\n",
      "\n",
      "[Memory]276.9s, 4.6min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1750838.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lr...\n",
      "\n",
      "[Memory]279.7s, 4.7min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1774890.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lin_svm...\n",
      "\n",
      "[Memory]300.1s, 5.0min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1782389.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rbf_svm...\n",
      "\n",
      "[Memory]349.0s, 5.8min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1777092.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rf...\n",
      "\n",
      "[Memory]1054.8s, 17.6min: Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1811910.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mlp...\n",
      "\n",
      "[Memory]1065.9s, 17.8min: Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1722886.11it/s]\n"
     ]
    }
   ],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "results = defaultdict(Result)\n",
    "\n",
    "for clf in clfs:\n",
    "    print(f'Training {clf.clf_id}...\\n')\n",
    "    \n",
    "    accuracy, precision, recall, f1, kappa = clf.train_and_evaluate(X_train, y_train, X_val, y_val, \n",
    "                                                          use_features=True, memory=memory)\n",
    "    \n",
    "    results[clf.clf_id].accuracy = accuracy \n",
    "    results[clf.clf_id].precision = precision\n",
    "    results[clf.clf_id].recall = recall\n",
    "    results[clf.clf_id].f1 = f1\n",
    "    results[clf.clf_id].kappa = kappa\n",
    "    \n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "regional-staff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.133, Kappa: 0.000\n",
      "\n",
      "gnb --- Accuracy: 0.611, Precision: 0.521, Recall: 0.500, F1: 0.509, Kappa: 0.404\n",
      "\n",
      "lr --- Accuracy: 0.622, Precision: 0.554, Recall: 0.469, F1: 0.498, Kappa: 0.405\n",
      "\n",
      "lin_svm --- Accuracy: 0.606, Precision: 0.540, Recall: 0.432, F1: 0.442, Kappa: 0.401\n",
      "\n",
      "rbf_svm --- Accuracy: 0.627, Precision: 0.559, Recall: 0.463, F1: 0.493, Kappa: 0.413\n",
      "\n",
      "rf --- Accuracy: 0.627, Precision: 0.549, Recall: 0.493, F1: 0.515, Kappa: 0.421\n",
      "\n",
      "mlp --- Accuracy: 0.624, Precision: 0.540, Recall: 0.498, F1: 0.516, Kappa: 0.422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in json.loads(Encoder().encode(results)).items():\n",
    "    accuracy = v['accuracy']\n",
    "    precision = v['precision']\n",
    "    recall = v['recall']\n",
    "    f1 = v['f1']\n",
    "    kappa = v['kappa']\n",
    "    \n",
    "    print(f'{k} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, Kappa: {kappa:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-preview",
   "metadata": {},
   "source": [
    "# Train classifiers without using additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adolescent-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpdw6ugtj8'\", use \"location='/tmp/tmpdw6ugtj8'\" instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dc...\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7fef961d1a50>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                               ('feature_processing',\n",
      "                                Pipeline(steps=[('features',\n",
      "                                                 FeatureUnion(transformer_list=[('phrase_length',\n",
      "                                                                                 Pipeline(steps=[('f1',\n",
      "                                                                                                  FunctionTransformer(func=<function get_phrase_length at 0x7fef9789d3b0>))]))]))]))]), \n",
      "209398             thing I ever saw written zeroes paycheck\n",
      "142562                   kinda dumb . And second shooting ?\n",
      "170792    one war movies focuses human interaction rathe...\n",
      "206415                              generally sad existence\n",
      "185194                   seem selfconsciously poetic forced\n",
      "                                ...                        \n",
      "13948                                              A comedy\n",
      "60202                                        With Notorious\n",
      "28661                               Frankly pretty stupid .\n",
      "127430                                 largely amateur cast\n",
      "77939                         unusual scifi character study\n",
      "Name: phrase_clean, Length: 126124, dtype: object, \n",
      "209398    3.0\n",
      "142562    2.0\n",
      "170792    3.0\n",
      "206415    2.0\n",
      "185194    2.0\n",
      "         ... \n",
      "13948     4.0\n",
      "60202     3.0\n",
      "28661     2.0\n",
      "127430    2.0\n",
      "77939     3.0\n",
      "Name: label_id, Length: 126124, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 2297243.74it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.64s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 273.5s, 4.6min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1988173.14it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.13290195526797016, Kappa: 0.0\n",
      "Training gnb...\n",
      "\n",
      "[Memory]277.7s, 4.6min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2001403.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnb --- Accuracy: 0.611, Precision: 0.521, Recall: 0.500, F1: 0.5094656485071292, Kappa: 0.40408449055598505\n",
      "Training lr...\n",
      "\n",
      "[Memory]280.4s, 4.7min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2022629.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr --- Accuracy: 0.622, Precision: 0.553, Recall: 0.469, F1: 0.49805320587337387, Kappa: 0.4048598366105536\n",
      "Training lin_svm...\n",
      "\n",
      "[Memory]305.2s, 5.1min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2037761.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin_svm --- Accuracy: 0.609, Precision: 0.535, Recall: 0.425, F1: 0.44951277498465536, Kappa: 0.37526375110956445\n",
      "Training rbf_svm...\n",
      "\n",
      "[Memory]356.3s, 5.9min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2191708.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf_svm --- Accuracy: 0.627, Precision: 0.559, Recall: 0.463, F1: 0.49267876349282114, Kappa: 0.4126594116546488\n",
      "Training rf...\n",
      "\n",
      "[Memory]1054.3s, 17.6min: Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2014994.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf --- Accuracy: 0.627, Precision: 0.548, Recall: 0.492, F1: 0.5140489877851053, Kappa: 0.4217551496418803\n",
      "Training mlp...\n",
      "\n",
      "[Memory]1065.6s, 17.8min: Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1984823.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp --- Accuracy: 0.623, Precision: 0.546, Recall: 0.483, F1: 0.5056438077978067, Kappa: 0.4180293487250639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaulate classifiers without additional features\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "results = defaultdict(Result)\n",
    "\n",
    "for clf in clfs:\n",
    "    print(f'Training {clf.clf_id}...\\n')\n",
    "    \n",
    "    accuracy, precision, recall, f1, kappa = clf.train_and_evaluate(X_train, y_train, X_val, y_val, \n",
    "                                                          use_features=True, memory=memory)\n",
    "    \n",
    "    print(f'{clf.clf_id} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1}, Kappa: {kappa}')\n",
    "    \n",
    "    results[clf.clf_id].accuracy = accuracy \n",
    "    results[clf.clf_id].precision = precision\n",
    "    results[clf.clf_id].recall = recall\n",
    "    results[clf.clf_id].f1 = f1\n",
    "    results[clf.clf_id].kappa = kappa\n",
    "    \n",
    "\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "choice-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.133, Kappa: 0.000\n",
      "\n",
      "gnb --- Accuracy: 0.611, Precision: 0.521, Recall: 0.500, F1: 0.509, Kappa: 0.404\n",
      "\n",
      "lr --- Accuracy: 0.622, Precision: 0.553, Recall: 0.469, F1: 0.498, Kappa: 0.405\n",
      "\n",
      "lin_svm --- Accuracy: 0.609, Precision: 0.535, Recall: 0.425, F1: 0.450, Kappa: 0.375\n",
      "\n",
      "rbf_svm --- Accuracy: 0.627, Precision: 0.559, Recall: 0.463, F1: 0.493, Kappa: 0.413\n",
      "\n",
      "rf --- Accuracy: 0.627, Precision: 0.548, Recall: 0.492, F1: 0.514, Kappa: 0.422\n",
      "\n",
      "mlp --- Accuracy: 0.623, Precision: 0.546, Recall: 0.483, F1: 0.506, Kappa: 0.418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in json.loads(Encoder().encode(results)).items():\n",
    "    accuracy = v['accuracy']\n",
    "    precision = v['precision']\n",
    "    recall = v['recall']\n",
    "    f1 = v['f1']\n",
    "    kappa = v['kappa']\n",
    "    \n",
    "    print(f'{k} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, Kappa: {kappa:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-firewall",
   "metadata": {},
   "source": [
    "# Best performing classifier on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "provincial-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n"
     ]
    }
   ],
   "source": [
    "# Dedup validation set to avoid adding any bias\n",
    "validate = DataLoader().dedup(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "criminal-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45449, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fixed-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = validate['phrase_clean'], validate['label_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "manual-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_val])\n",
    "y = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "worst-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171573,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "desperate-excess",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmp2q0p9qnv'\", use \"location='/tmp/tmp2q0p9qnv'\" instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7fe4c50ba350>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                               ('feature_processing',\n",
      "                                Pipeline(steps=[('features',\n",
      "                                                 FeatureUnion(transformer_list=[('phrase_length',\n",
      "                                                                                 Pipeline(steps=[('f1',\n",
      "                                                                                                  FunctionTransformer(func=<function get_phrase_length at 0x7fe4938ee830>))]))]))]))]), \n",
      "211997                  story film portrays\n",
      "215202                            think one\n",
      "201915        Duke something theatrical air\n",
      "212237                            sweetness\n",
      "178664       question firmly held positions\n",
      "                        ...                \n",
      "172980                               paeans\n",
      "190797                      dramatic scenes\n",
      "115760                     blunt exposition\n",
      "235790    without bludgeoning audience head\n",
      "108206       engaged ferocious debate years\n",
      "Name: phrase_clean, Length: 171573, dtype: object, \n",
      "211997    3.0\n",
      "215202    3.0\n",
      "201915    3.0\n",
      "212237    4.0\n",
      "178664    3.0\n",
      "         ... \n",
      "172980    3.0\n",
      "190797    3.0\n",
      "115760    4.0\n",
      "235790    4.0\n",
      "108206    4.0\n",
      "Name: label_id, Length: 171573, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171573/171573 [00:00<00:00, 2234193.69it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.77s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 350.6s, 5.8min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1807092.19it/s]\n"
     ]
    }
   ],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "accuracy, precision, recall, f1, kappa = mlp.train_and_evaluate(X, y, \n",
    "                                                                X_test, y_test, \n",
    "                                                                use_features=True, \n",
    "                                                                memory=memory)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "handmade-hunger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6289024647145566,\n",
       " 0.5653075993063515,\n",
       " 0.4924972970936013,\n",
       " 0.5181950316262732,\n",
       " 0.4295395095522072)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-emperor",
   "metadata": {},
   "source": [
    "# Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-egypt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
