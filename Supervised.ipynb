{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "raising-ribbon",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amino-psychiatry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s21/s2125219/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from preprocessing import Preprocessor\n",
    "from data_loader import DataLoader, Encoder\n",
    "from helpers import select_n_components, pos_check\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-still",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "determined-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3454/239073 [00:00<00:06, 34534.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239073/239073 [00:05<00:00, 42436.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = DataLoader().create_dataframe(preprocess=True, split=True, remove_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "younger-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_clean</th>\n",
       "      <th>sentiment_val</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190264</th>\n",
       "      <td>212000</td>\n",
       "      <td>so second-rate</td>\n",
       "      <td>secondrate</td>\n",
       "      <td>0.26389</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120749</th>\n",
       "      <td>121243</td>\n",
       "      <td>go with this claustrophobic concept</td>\n",
       "      <td>go claustrophobic concept</td>\n",
       "      <td>0.36111</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189861</th>\n",
       "      <td>211915</td>\n",
       "      <td>so completely</td>\n",
       "      <td>completely</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106888</th>\n",
       "      <td>6123</td>\n",
       "      <td>ease and confidence</td>\n",
       "      <td>ease confidence</td>\n",
       "      <td>0.65278</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223674</th>\n",
       "      <td>42917</td>\n",
       "      <td>treachery and</td>\n",
       "      <td>treachery</td>\n",
       "      <td>0.45833</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        phrase_id                               phrase  \\\n",
       "190264     212000                       so second-rate   \n",
       "120749     121243  go with this claustrophobic concept   \n",
       "189861     211915                        so completely   \n",
       "106888       6123                  ease and confidence   \n",
       "223674      42917                        treachery and   \n",
       "\n",
       "                     phrase_clean  sentiment_val  label_id     label  \\\n",
       "190264                 secondrate        0.26389       2.0  Negative   \n",
       "120749  go claustrophobic concept        0.36111       2.0  Negative   \n",
       "189861                 completely        0.50000       3.0   Neutral   \n",
       "106888            ease confidence        0.65278       4.0  Positive   \n",
       "223674                  treachery        0.45833       3.0   Neutral   \n",
       "\n",
       "        word_count  \n",
       "190264           1  \n",
       "120749           3  \n",
       "189861           1  \n",
       "106888           2  \n",
       "223674           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sticky-berkeley",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126124, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-porcelain",
   "metadata": {},
   "source": [
    "# Train, test, dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "placed-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['phrase_clean'], train['label_id']\n",
    "X_val, y_val = validate['phrase_clean'], validate['label_id']\n",
    "X_test, y_test = test['phrase_clean'], test['label_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-abraham",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection\n",
    "\n",
    "- Features to include:\n",
    "\n",
    "    - phrase length\n",
    "    - punctuation count\n",
    "    - capital letters count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unusual-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_count = lambda l1, l2: sum([1 for x in l1 if x in l2])\n",
    "caps_count = lambda l1: sum([1 for x in l1 if x.isupper()])\n",
    "\n",
    "def get_phrase_length(text):\n",
    "    return np.array([len(t) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_punct(text):\n",
    "    return np.array([punct_count(t, set(string.punctuation)) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_caps(text):\n",
    "    return np.array([caps_count(t) for t in tqdm(text)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "romance-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', \n",
    "                             max_features=10000, \n",
    "                             use_idf=True, \n",
    "                             ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-communications",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "meaningful-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.accuracy = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "        self.f1 = 0\n",
    "        self.kappa = 0\n",
    "        \n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        print('Making dense transformation...\\n')\n",
    "        return X.todense()\n",
    "\n",
    "class ClassificationPipeline():\n",
    "    def __init__(self, clf_id, clf, vectorizer, feature_processing, pipe=None):\n",
    "        self.pipe = pipe \n",
    "        self.clf_id = clf_id \n",
    "        self.clf = clf\n",
    "        self.vectorizer = vectorizer\n",
    "        self.feature_processing = feature_processing\n",
    "                   \n",
    "    def create_feature_pipeline(self, memory):\n",
    "        self.pipe = Pipeline([\n",
    "            ('feature_pipeline', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', self.vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4)),\n",
    "                ])),\n",
    "                ('feature_processing', self.feature_processing)\n",
    "            ])),\n",
    "            (self.clf_id, self.clf)\n",
    "        ], memory=memory)\n",
    "    \n",
    "    def create_pipeline(self, memory):\n",
    "        self.pipe = Pipeline([\n",
    "            ('feature_pipeline', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', self.vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4)),\n",
    "                ])),\n",
    "            ])),\n",
    "            (self.clf_id, self.clf)\n",
    "        ], memory=memory)\n",
    "            \n",
    "    def train_and_evaluate(self, X_train, y_train, X_val, y_val, memory, use_features=True):\n",
    "        if use_features:\n",
    "            self.create_feature_pipeline(memory=memory)\n",
    "        else:\n",
    "            self.create_pipeline(memory=memory)\n",
    "            \n",
    "        self.pipe.fit(X_train, y_train)\n",
    "        preds = self.pipe.predict(X_val)\n",
    "        \n",
    "        accuracy = accuracy_score(y_val, preds)\n",
    "        precision = precision_score(y_val, preds, average='macro')\n",
    "        recall = recall_score(y_val, preds, average='macro')\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        kappa = cohen_kappa_score(y_val, preds)\n",
    "        \n",
    "        return accuracy, precision, recall, f1, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aboriginal-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "features = FeatureUnion([\n",
    "    ('phrase_length', Pipeline([\n",
    "        ('f1', FunctionTransformer(get_phrase_length, validate=False))]\n",
    "    )),\n",
    "#     ('num_punct', Pipeline([\n",
    "#         ('f2', FunctionTransformer(get_num_punct, validate=False))]\n",
    "#     )),\n",
    "#     ('num_caps', Pipeline([\n",
    "#         ('f3', FunctionTransformer(get_num_caps, validate=False))\n",
    "#     ]))\n",
    "])\n",
    "\n",
    "feature_processing = Pipeline([('features', features)])\n",
    "\n",
    "# Classifiers\n",
    "dc = ClassificationPipeline(clf_id='dc', \n",
    "                            clf=DummyClassifier(strategy='most_frequent'),\n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "gnb = ClassificationPipeline(clf_id='gnb',\n",
    "                            clf=GaussianNB(),\n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "lr = ClassificationPipeline(clf_id='lr', \n",
    "                           clf=LogisticRegression(max_iter=10000),\n",
    "                           vectorizer=tfidf_vect,\n",
    "                           feature_processing=feature_processing)\n",
    "lin_svm = ClassificationPipeline(clf_id='lin_svm', \n",
    "                                 clf=LinearSVC(),\n",
    "                                 vectorizer=tfidf_vect,\n",
    "                                 feature_processing=feature_processing)\n",
    "rbf_svm = ClassificationPipeline(clf_id='rbf_svm', \n",
    "                                 clf=SVC(kernel='rbf'), \n",
    "                                 vectorizer=tfidf_vect,\n",
    "                                 feature_processing=feature_processing)\n",
    "rf = ClassificationPipeline(clf_id='rf', \n",
    "                            clf=RandomForestClassifier(max_depth=10, n_estimators=50), \n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "mlp = ClassificationPipeline(clf_id='mlp', \n",
    "                             clf=MLPClassifier(max_iter=800),\n",
    "                             vectorizer=tfidf_vect,\n",
    "                             feature_processing=feature_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acknowledged-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate classifiers using additional features\n",
    "clfs = [dc, gnb, lr, lin_svm, rbf_svm, rf, mlp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-pakistan",
   "metadata": {},
   "source": [
    "# Train classifiers using additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "italic-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpdtl1wrtb'\", use \"location='/tmp/tmpdtl1wrtb'\" instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dc...\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7fc4cd151110>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                             ..., \n",
      "190264                   secondrate\n",
      "120749    go claustrophobic concept\n",
      "189861                   completely\n",
      "106888              ease confidence\n",
      "223674                    treachery\n",
      "                    ...            \n",
      "216382                       reason\n",
      "103827       discouraging let slide\n",
      "174300     perpetrating Patch Adams\n",
      "117911                 stark desert\n",
      "22823               Callie Khouri .\n",
      "Name: phrase_clean, Length: 126124, dtype: object, \n",
      "190264    2.0\n",
      "120749    2.0\n",
      "189861    3.0\n",
      "106888    4.0\n",
      "223674    3.0\n",
      "         ... \n",
      "216382    3.0\n",
      "103827    3.0\n",
      "174300    3.0\n",
      "117911    4.0\n",
      "22823     3.0\n",
      "Name: label_id, Length: 126124, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 2175496.36it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 425104.35it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 499577.30it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.63s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 252.9s, 4.2min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2281414.55it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 410605.88it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 447261.02it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gnb...\n",
      "\n",
      "[Memory]257.3s, 4.3min  : Loading _fit_transform_one from /tmp/tmpdtl1wrtb/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2304199.92it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 401877.16it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 491283.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lr...\n",
      "\n",
      "[Memory]260.2s, 4.3min  : Loading _fit_transform_one from /tmp/tmpdtl1wrtb/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1821058.51it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 405182.85it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 470054.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lin_svm...\n",
      "\n",
      "[Memory]277.2s, 4.6min  : Loading _fit_transform_one from /tmp/tmpdtl1wrtb/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1856028.59it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 406544.64it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 490732.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rbf_svm...\n",
      "\n",
      "[Memory]334.9s, 5.6min  : Loading _fit_transform_one from /tmp/tmpdtl1wrtb/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1844884.37it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 404043.22it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 479240.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rf...\n",
      "\n",
      "[Memory]1059.1s, 17.7min: Loading _fit_transform_one from /tmp/tmpdtl1wrtb/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1985160.04it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 397256.98it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 450708.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mlp...\n",
      "\n",
      "[Memory]1068.8s, 17.8min: Loading _fit_transform_one from /tmp/tmpdtl1wrtb/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2247066.91it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 411023.76it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 450236.56it/s]\n"
     ]
    }
   ],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "results = defaultdict(Result)\n",
    "\n",
    "for clf in clfs:\n",
    "    print(f'Training {clf.clf_id}...\\n')\n",
    "    \n",
    "    accuracy, precision, recall, f1, kappa = clf.train_and_evaluate(X_train, y_train, X_val, y_val, \n",
    "                                                          use_features=True, memory=memory)\n",
    "    \n",
    "    results[clf.clf_id].accuracy = accuracy \n",
    "    results[clf.clf_id].precision = precision\n",
    "    results[clf.clf_id].recall = recall\n",
    "    results[clf.clf_id].f1 = f1\n",
    "    results[clf.clf_id].kappa = kappa\n",
    "    \n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opening-narrative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.133, Kappa: 0.000\n",
      "\n",
      "gnb --- Accuracy: 0.602, Precision: 0.501, Recall: 0.493, F1: 0.495, Kappa: 0.391\n",
      "\n",
      "lr --- Accuracy: 0.623, Precision: 0.554, Recall: 0.469, F1: 0.499, Kappa: 0.406\n",
      "\n",
      "lin_svm --- Accuracy: 0.574, Precision: 0.559, Recall: 0.377, F1: 0.378, Kappa: 0.343\n",
      "\n",
      "rbf_svm --- Accuracy: 0.628, Precision: 0.560, Recall: 0.465, F1: 0.494, Kappa: 0.415\n",
      "\n",
      "rf --- Accuracy: 0.626, Precision: 0.549, Recall: 0.487, F1: 0.510, Kappa: 0.419\n",
      "\n",
      "mlp --- Accuracy: 0.622, Precision: 0.541, Recall: 0.495, F1: 0.513, Kappa: 0.419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in json.loads(Encoder().encode(results)).items():\n",
    "    accuracy = v['accuracy']\n",
    "    precision = v['precision']\n",
    "    recall = v['recall']\n",
    "    f1 = v['f1']\n",
    "    kappa = v['kappa']\n",
    "    \n",
    "    print(f'{k} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, Kappa: {kappa:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-newsletter",
   "metadata": {},
   "source": [
    "# Train classifiers without using additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unlikely-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpnm_h1at5'\", use \"location='/tmp/tmpnm_h1at5'\" instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dc...\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7fc4ce7ffb50>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                             ..., \n",
      "190264                   secondrate\n",
      "120749    go claustrophobic concept\n",
      "189861                   completely\n",
      "106888              ease confidence\n",
      "223674                    treachery\n",
      "                    ...            \n",
      "216382                       reason\n",
      "103827       discouraging let slide\n",
      "174300     perpetrating Patch Adams\n",
      "117911                 stark desert\n",
      "22823               Callie Khouri .\n",
      "Name: phrase_clean, Length: 126124, dtype: object, \n",
      "190264    2.0\n",
      "120749    2.0\n",
      "189861    3.0\n",
      "106888    4.0\n",
      "223674    3.0\n",
      "         ... \n",
      "216382    3.0\n",
      "103827    3.0\n",
      "174300    3.0\n",
      "117911    4.0\n",
      "22823     3.0\n",
      "Name: label_id, Length: 126124, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 2151737.03it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 417352.97it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 485675.75it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.68s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 247.1s, 4.1min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2224149.18it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 407646.69it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 441582.06it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.13290195526797016, Kappa: 0.0\n",
      "Training gnb...\n",
      "\n",
      "[Memory]251.5s, 4.2min  : Loading _fit_transform_one from /tmp/tmpnm_h1at5/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2247777.22it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 410644.83it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 478028.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnb --- Accuracy: 0.602, Precision: 0.501, Recall: 0.493, F1: 0.4946757953164787, Kappa: 0.39078783708428255\n",
      "Training lr...\n",
      "\n",
      "[Memory]254.5s, 4.2min  : Loading _fit_transform_one from /tmp/tmpnm_h1at5/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2220849.63it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 404847.53it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 436102.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr --- Accuracy: 0.623, Precision: 0.554, Recall: 0.469, F1: 0.49862062348428465, Kappa: 0.405959194323282\n",
      "Training lin_svm...\n",
      "\n",
      "[Memory]270.7s, 4.5min  : Loading _fit_transform_one from /tmp/tmpnm_h1at5/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1840926.93it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 405135.03it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 474522.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin_svm --- Accuracy: 0.610, Precision: 0.560, Recall: 0.416, F1: 0.43706289160450995, Kappa: 0.38530510430823295\n",
      "Training rbf_svm...\n",
      "\n",
      "[Memory]330.6s, 5.5min  : Loading _fit_transform_one from /tmp/tmpnm_h1at5/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2256694.14it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 409873.87it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 463852.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf_svm --- Accuracy: 0.628, Precision: 0.560, Recall: 0.465, F1: 0.49421423105242646, Kappa: 0.4148779886577302\n",
      "Training rf...\n",
      "\n",
      "[Memory]1045.3s, 17.4min: Loading _fit_transform_one from /tmp/tmpnm_h1at5/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1812537.42it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 408850.42it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 477564.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf --- Accuracy: 0.628, Precision: 0.552, Recall: 0.489, F1: 0.5127251485666179, Kappa: 0.42219492158266414\n",
      "Training mlp...\n",
      "\n",
      "[Memory]1055.2s, 17.6min: Loading _fit_transform_one from /tmp/tmpnm_h1at5/joblib/sklearn/pipeline/_fit_transform_one/6e7cb0abd50c5cf851df923d78866d86\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1424457.96it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 405753.42it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 475003.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp --- Accuracy: 0.625, Precision: 0.547, Recall: 0.489, F1: 0.5113847957621365, Kappa: 0.42154886706205574\n"
     ]
    }
   ],
   "source": [
    "# Train and evaulate classifiers without additional features\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "results = defaultdict(Result)\n",
    "\n",
    "for clf in clfs:\n",
    "    print(f'Training {clf.clf_id}...\\n')\n",
    "    \n",
    "    accuracy, precision, recall, f1, kappa = clf.train_and_evaluate(X_train, y_train, X_val, y_val, \n",
    "                                                          use_features=True, memory=memory)\n",
    "    \n",
    "    print(f'{clf.clf_id} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1}, Kappa: {kappa}')\n",
    "    \n",
    "    results[clf.clf_id].accuracy = accuracy \n",
    "    results[clf.clf_id].precision = precision\n",
    "    results[clf.clf_id].recall = recall\n",
    "    results[clf.clf_id].f1 = f1\n",
    "    results[clf.clf_id].kappa = kappa\n",
    "    \n",
    "\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "under-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.133, Kappa: 0.000\n",
      "\n",
      "gnb --- Accuracy: 0.602, Precision: 0.501, Recall: 0.493, F1: 0.495, Kappa: 0.391\n",
      "\n",
      "lr --- Accuracy: 0.623, Precision: 0.554, Recall: 0.469, F1: 0.499, Kappa: 0.406\n",
      "\n",
      "lin_svm --- Accuracy: 0.610, Precision: 0.560, Recall: 0.416, F1: 0.437, Kappa: 0.385\n",
      "\n",
      "rbf_svm --- Accuracy: 0.628, Precision: 0.560, Recall: 0.465, F1: 0.494, Kappa: 0.415\n",
      "\n",
      "rf --- Accuracy: 0.628, Precision: 0.552, Recall: 0.489, F1: 0.513, Kappa: 0.422\n",
      "\n",
      "mlp --- Accuracy: 0.625, Precision: 0.547, Recall: 0.489, F1: 0.511, Kappa: 0.422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in json.loads(Encoder().encode(results)).items():\n",
    "    accuracy = v['accuracy']\n",
    "    precision = v['precision']\n",
    "    recall = v['recall']\n",
    "    f1 = v['f1']\n",
    "    kappa = v['kappa']\n",
    "    \n",
    "    print(f'{k} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, Kappa: {kappa:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-bouquet",
   "metadata": {},
   "source": [
    "# Best performing classifier on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "chief-tulsa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n"
     ]
    }
   ],
   "source": [
    "# Dedup validation set to avoid adding any bias\n",
    "validate = DataLoader().dedup(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interstate-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45449, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "inappropriate-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = validate['phrase_clean'], validate['label_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "surrounded-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_val])\n",
    "y = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ordinary-reduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171573,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "organizational-portable",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpp4slgsi_'\", use \"location='/tmp/tmpp4slgsi_'\" instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7fc4cd305f50>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                               ('feature_processing',\n",
      "                                Pipeline(steps=[('features',\n",
      "                                                 FeatureUnion(transformer_list=[('phrase_length',\n",
      "                                                                                 Pipeline(steps=[('f1',\n",
      "                                                                                                  FunctionTransformer(func=<function get_phrase_length at 0x7fc4ca220ef0>))]))]))]))]), \n",
      "190264                           secondrate\n",
      "120749            go claustrophobic concept\n",
      "189861                           completely\n",
      "106888                      ease confidence\n",
      "223674                            treachery\n",
      "                        ...                \n",
      "172980                               paeans\n",
      "190797                      dramatic scenes\n",
      "115760                     blunt exposition\n",
      "235790    without bludgeoning audience head\n",
      "108206       engaged ferocious debate years\n",
      "Name: phrase_clean, Length: 171573, dtype: object, \n",
      "190264    2.0\n",
      "120749    2.0\n",
      "189861    3.0\n",
      "106888    4.0\n",
      "223674    3.0\n",
      "         ... \n",
      "172980    3.0\n",
      "190797    3.0\n",
      "115760    4.0\n",
      "235790    4.0\n",
      "108206    4.0\n",
      "Name: label_id, Length: 171573, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171573/171573 [00:00<00:00, 1930854.09it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.89s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 411.5s, 6.9min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1688492.10it/s]\n"
     ]
    }
   ],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "accuracy, precision, recall, f1, kappa = rf.train_and_evaluate(X, y, \n",
    "                                                                X_test, y_test, \n",
    "                                                                use_features=True, \n",
    "                                                                memory=memory)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "operational-tuner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6350747840741521,\n",
       " 0.5729196788805456,\n",
       " 0.5003701461077453,\n",
       " 0.5275887782465907,\n",
       " 0.434207220408908)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-dominant",
   "metadata": {},
   "source": [
    "# Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-filter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
