{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assisted-dividend",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinated-civilian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /afs/inf.ed.ac.uk/user/s21/s2125219/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from preprocessing import Preprocessor\n",
    "from data_loader import DataLoader, Encoder\n",
    "from helpers import select_n_components, pos_check\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-briefing",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "structural-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3508/239073 [00:00<00:06, 35079.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239073/239073 [00:05<00:00, 42710.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = DataLoader().create_dataframe(preprocess=True, split=True, remove_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threaded-simulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_clean</th>\n",
       "      <th>sentiment_val</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147608</th>\n",
       "      <td>42267</td>\n",
       "      <td>know the ` truth ' about this man , while deco...</td>\n",
       "      <td>know truth man deconstructing format biography...</td>\n",
       "      <td>0.55556</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143906</th>\n",
       "      <td>33690</td>\n",
       "      <td>it mostly worth the trip</td>\n",
       "      <td>mostly worth trip</td>\n",
       "      <td>0.56944</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90526</th>\n",
       "      <td>194916</td>\n",
       "      <td>branched out into their own pseudo-witty copyc...</td>\n",
       "      <td>branched pseudowitty copycat interpretations</td>\n",
       "      <td>0.34722</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41158</th>\n",
       "      <td>147570</td>\n",
       "      <td>Murder by Numbers just does n't add up .</td>\n",
       "      <td>Murder Numbers nt add .</td>\n",
       "      <td>0.38889</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164045</th>\n",
       "      <td>234333</td>\n",
       "      <td>of `` Minority Report</td>\n",
       "      <td>Minority Report</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        phrase_id                                             phrase  \\\n",
       "147608      42267  know the ` truth ' about this man , while deco...   \n",
       "143906      33690                           it mostly worth the trip   \n",
       "90526      194916  branched out into their own pseudo-witty copyc...   \n",
       "41158      147570           Murder by Numbers just does n't add up .   \n",
       "164045     234333                              of `` Minority Report   \n",
       "\n",
       "                                             phrase_clean  sentiment_val  \\\n",
       "147608  know truth man deconstructing format biography...        0.55556   \n",
       "143906                                  mostly worth trip        0.56944   \n",
       "90526        branched pseudowitty copycat interpretations        0.34722   \n",
       "41158                             Murder Numbers nt add .        0.38889   \n",
       "164045                                    Minority Report        0.50000   \n",
       "\n",
       "        label_id     label  word_count  \n",
       "147608       3.0   Neutral           7  \n",
       "143906       3.0   Neutral           3  \n",
       "90526        2.0  Negative           4  \n",
       "41158        2.0  Negative           5  \n",
       "164045       3.0   Neutral           2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "substantial-toyota",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126124, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-textbook",
   "metadata": {},
   "source": [
    "# Train, test, dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scenic-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['phrase_clean'], train['label_id']\n",
    "X_val, y_val = validate['phrase_clean'], validate['label_id']\n",
    "X_test, y_test = test['phrase_clean'], test['label_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-persian",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection\n",
    "\n",
    "- Features to include:\n",
    "\n",
    "    - phrase length\n",
    "    - punctuation count\n",
    "    - capital letters count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "focal-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_count = lambda l1, l2: sum([1 for x in l1 if x in l2])\n",
    "caps_count = lambda l1: sum([1 for x in l1 if x.isupper()])\n",
    "\n",
    "def get_phrase_length(text):\n",
    "    return np.array([len(t) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_punct(text):\n",
    "    return np.array([punct_count(t, set(string.punctuation)) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_caps(text):\n",
    "    return np.array([caps_count(t) for t in tqdm(text)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "portuguese-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', \n",
    "                             max_features=10000, \n",
    "                             use_idf=True, \n",
    "                             ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-beatles",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nasty-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.accuracy = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "        self.f1 = 0\n",
    "        self.kappa = 0\n",
    "        \n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        print('Making dense transformation...\\n')\n",
    "        return X.todense()\n",
    "\n",
    "class ClassificationPipeline():\n",
    "    def __init__(self, clf_id, clf, vectorizer, feature_processing, pipe=None):\n",
    "        self.pipe = pipe \n",
    "        self.clf_id = clf_id \n",
    "        self.clf = clf\n",
    "        self.vectorizer = vectorizer\n",
    "        self.feature_processing = feature_processing\n",
    "                   \n",
    "    def create_feature_pipeline(self, memory):\n",
    "        self.pipe = Pipeline([\n",
    "            ('feature_pipeline', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', self.vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4)),\n",
    "                ])),\n",
    "                ('feature_processing', self.feature_processing)\n",
    "            ])),\n",
    "            (self.clf_id, self.clf)\n",
    "        ], memory=memory)\n",
    "    \n",
    "    def create_pipeline(self, memory):\n",
    "        self.pipe = Pipeline([\n",
    "            ('feature_pipeline', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', self.vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4)),\n",
    "                ])),\n",
    "            ])),\n",
    "            (self.clf_id, self.clf)\n",
    "        ], memory=memory)\n",
    "            \n",
    "    def train_and_evaluate(self, X_train, y_train, X_val, y_val, memory, use_features=True):\n",
    "        if use_features:\n",
    "            self.create_feature_pipeline(memory=memory)\n",
    "        else:\n",
    "            self.create_pipeline(memory=memory)\n",
    "            \n",
    "        self.pipe.fit(X_train, y_train)\n",
    "        preds = self.pipe.predict(X_val)\n",
    "        \n",
    "        accuracy = accuracy_score(y_val, preds)\n",
    "        precision = precision_score(y_val, preds, average='macro')\n",
    "        recall = recall_score(y_val, preds, average='macro')\n",
    "        f1 = f1_score(y_val, preds, average='macro')\n",
    "        kappa = cohen_kappa_score(y_val, preds)\n",
    "        \n",
    "        return accuracy, precision, recall, f1, kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "piano-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "features = FeatureUnion([\n",
    "    ('phrase_length', Pipeline([\n",
    "        ('f1', FunctionTransformer(get_phrase_length, validate=False))]\n",
    "    ))\n",
    "])\n",
    "\n",
    "feature_processing = Pipeline([('features', features)])\n",
    "\n",
    "# Classifiers\n",
    "dc = ClassificationPipeline(clf_id='dc', \n",
    "                            clf=DummyClassifier(strategy='most_frequent'),\n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "gnb = ClassificationPipeline(clf_id='gnb',\n",
    "                            clf=GaussianNB(),\n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "lr = ClassificationPipeline(clf_id='lr', \n",
    "                           clf=LogisticRegression(max_iter=10000),\n",
    "                           vectorizer=tfidf_vect,\n",
    "                           feature_processing=feature_processing)\n",
    "lin_svm = ClassificationPipeline(clf_id='lin_svm', \n",
    "                                 clf=LinearSVC(),\n",
    "                                 vectorizer=tfidf_vect,\n",
    "                                 feature_processing=feature_processing)\n",
    "rbf_svm = ClassificationPipeline(clf_id='rbf_svm', \n",
    "                                 clf=SVC(kernel='rbf'), \n",
    "                                 vectorizer=tfidf_vect,\n",
    "                                 feature_processing=feature_processing)\n",
    "rf = ClassificationPipeline(clf_id='rf', \n",
    "                            clf=RandomForestClassifier(max_depth=10, n_estimators=50), \n",
    "                            vectorizer=tfidf_vect,\n",
    "                            feature_processing=feature_processing)\n",
    "mlp = ClassificationPipeline(clf_id='mlp', \n",
    "                             clf=MLPClassifier(max_iter=800),\n",
    "                             vectorizer=tfidf_vect,\n",
    "                             feature_processing=feature_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unlikely-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate classifiers using additional features\n",
    "clfs = [dc, gnb, lr, lin_svm, rbf_svm, rf, mlp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-raising",
   "metadata": {},
   "source": [
    "# Train classifiers using additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cheap-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmp2msbi13o'\", use \"location='/tmp/tmp2msbi13o'\" instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dc...\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7f3be27c4190>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                               ('feature_processing',\n",
      "                                Pipeline(steps=[('features',\n",
      "                                                 FeatureUnion(transformer_list=[('phrase_length',\n",
      "                                                                                 Pipeline(steps=[('f1',\n",
      "                                                                                                  FunctionTransformer(func=<function get_phrase_length at 0x7f3be20a3dd0>))]))]))]))]), \n",
      "148695                              learning cultural clash\n",
      "166674                     political prisoners persecutions\n",
      "135059                                             intrigue\n",
      "206208                                                 foot\n",
      "178384    put gentle laughs equally gentle sentiments bu...\n",
      "                                ...                        \n",
      "107618                                        emerges movie\n",
      "166523    better films especially Seven director William...\n",
      "208626                                               mother\n",
      "219939                                               edited\n",
      "224275    try balance pointed often incisive satire unab...\n",
      "Name: phrase_clean, Length: 126124, dtype: object, \n",
      "148695    3.0\n",
      "166674    3.0\n",
      "135059    3.0\n",
      "206208    3.0\n",
      "178384    5.0\n",
      "         ... \n",
      "107618    3.0\n",
      "166523    2.0\n",
      "208626    3.0\n",
      "219939    3.0\n",
      "224275    5.0\n",
      "Name: label_id, Length: 126124, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1941015.41it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.65s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 272.8s, 4.5min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1731847.77it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training gnb...\n",
      "\n",
      "[Memory]276.9s, 4.6min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1750838.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lr...\n",
      "\n",
      "[Memory]279.7s, 4.7min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1774890.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lin_svm...\n",
      "\n",
      "[Memory]300.1s, 5.0min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1782389.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rbf_svm...\n",
      "\n",
      "[Memory]349.0s, 5.8min  : Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1777092.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rf...\n",
      "\n",
      "[Memory]1054.8s, 17.6min: Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1811910.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mlp...\n",
      "\n",
      "[Memory]1065.9s, 17.8min: Loading _fit_transform_one from /tmp/tmp2msbi13o/joblib/sklearn/pipeline/_fit_transform_one/3d6a0d23f84ddf42fe6ce6bacd6ab287\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1722886.11it/s]\n"
     ]
    }
   ],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "results = defaultdict(Result)\n",
    "\n",
    "for clf in clfs:\n",
    "    print(f'Training {clf.clf_id}...\\n')\n",
    "    \n",
    "    accuracy, precision, recall, f1, kappa = clf.train_and_evaluate(X_train, y_train, X_val, y_val, \n",
    "                                                          use_features=True, memory=memory)\n",
    "    \n",
    "    results[clf.clf_id].accuracy = accuracy \n",
    "    results[clf.clf_id].precision = precision\n",
    "    results[clf.clf_id].recall = recall\n",
    "    results[clf.clf_id].f1 = f1\n",
    "    results[clf.clf_id].kappa = kappa\n",
    "    \n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "numeric-crisis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.133, Kappa: 0.000\n",
      "\n",
      "gnb --- Accuracy: 0.611, Precision: 0.521, Recall: 0.500, F1: 0.509, Kappa: 0.404\n",
      "\n",
      "lr --- Accuracy: 0.622, Precision: 0.554, Recall: 0.469, F1: 0.498, Kappa: 0.405\n",
      "\n",
      "lin_svm --- Accuracy: 0.606, Precision: 0.540, Recall: 0.432, F1: 0.442, Kappa: 0.401\n",
      "\n",
      "rbf_svm --- Accuracy: 0.627, Precision: 0.559, Recall: 0.463, F1: 0.493, Kappa: 0.413\n",
      "\n",
      "rf --- Accuracy: 0.627, Precision: 0.549, Recall: 0.493, F1: 0.515, Kappa: 0.421\n",
      "\n",
      "mlp --- Accuracy: 0.624, Precision: 0.540, Recall: 0.498, F1: 0.516, Kappa: 0.422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in json.loads(Encoder().encode(results)).items():\n",
    "    accuracy = v['accuracy']\n",
    "    precision = v['precision']\n",
    "    recall = v['recall']\n",
    "    f1 = v['f1']\n",
    "    kappa = v['kappa']\n",
    "    \n",
    "    print(f'{k} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, Kappa: {kappa:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-desperate",
   "metadata": {},
   "source": [
    "# Train classifiers without using additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acoustic-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpdw6ugtj8'\", use \"location='/tmp/tmpdw6ugtj8'\" instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dc...\n",
      "\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7fef961d1a50>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                               ('feature_processing',\n",
      "                                Pipeline(steps=[('features',\n",
      "                                                 FeatureUnion(transformer_list=[('phrase_length',\n",
      "                                                                                 Pipeline(steps=[('f1',\n",
      "                                                                                                  FunctionTransformer(func=<function get_phrase_length at 0x7fef9789d3b0>))]))]))]))]), \n",
      "209398             thing I ever saw written zeroes paycheck\n",
      "142562                   kinda dumb . And second shooting ?\n",
      "170792    one war movies focuses human interaction rathe...\n",
      "206415                              generally sad existence\n",
      "185194                   seem selfconsciously poetic forced\n",
      "                                ...                        \n",
      "13948                                              A comedy\n",
      "60202                                        With Notorious\n",
      "28661                               Frankly pretty stupid .\n",
      "127430                                 largely amateur cast\n",
      "77939                         unusual scifi character study\n",
      "Name: phrase_clean, Length: 126124, dtype: object, \n",
      "209398    3.0\n",
      "142562    2.0\n",
      "170792    3.0\n",
      "206415    2.0\n",
      "185194    2.0\n",
      "         ... \n",
      "13948     4.0\n",
      "60202     3.0\n",
      "28661     2.0\n",
      "127430    2.0\n",
      "77939     3.0\n",
      "Name: label_id, Length: 126124, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 2297243.74it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.64s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 273.5s, 4.6min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1988173.14it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.13290195526797016, Kappa: 0.0\n",
      "Training gnb...\n",
      "\n",
      "[Memory]277.7s, 4.6min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2001403.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnb --- Accuracy: 0.611, Precision: 0.521, Recall: 0.500, F1: 0.5094656485071292, Kappa: 0.40408449055598505\n",
      "Training lr...\n",
      "\n",
      "[Memory]280.4s, 4.7min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2022629.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr --- Accuracy: 0.622, Precision: 0.553, Recall: 0.469, F1: 0.49805320587337387, Kappa: 0.4048598366105536\n",
      "Training lin_svm...\n",
      "\n",
      "[Memory]305.2s, 5.1min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2037761.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin_svm --- Accuracy: 0.609, Precision: 0.535, Recall: 0.425, F1: 0.44951277498465536, Kappa: 0.37526375110956445\n",
      "Training rbf_svm...\n",
      "\n",
      "[Memory]356.3s, 5.9min  : Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2191708.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf_svm --- Accuracy: 0.627, Precision: 0.559, Recall: 0.463, F1: 0.49267876349282114, Kappa: 0.4126594116546488\n",
      "Training rf...\n",
      "\n",
      "[Memory]1054.3s, 17.6min: Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 2014994.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf --- Accuracy: 0.627, Precision: 0.548, Recall: 0.492, F1: 0.5140489877851053, Kappa: 0.4217551496418803\n",
      "Training mlp...\n",
      "\n",
      "[Memory]1065.6s, 17.8min: Loading _fit_transform_one from /tmp/tmpdw6ugtj8/joblib/sklearn/pipeline/_fit_transform_one/d225e5ba4ef299475900aa4c8daee4a8\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1984823.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp --- Accuracy: 0.623, Precision: 0.546, Recall: 0.483, F1: 0.5056438077978067, Kappa: 0.4180293487250639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaulate classifiers without additional features\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "results = defaultdict(Result)\n",
    "\n",
    "for clf in clfs:\n",
    "    print(f'Training {clf.clf_id}...\\n')\n",
    "    \n",
    "    accuracy, precision, recall, f1, kappa = clf.train_and_evaluate(X_train, y_train, X_val, y_val, \n",
    "                                                          use_features=True, memory=memory)\n",
    "    \n",
    "    print(f'{clf.clf_id} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1}, Kappa: {kappa}')\n",
    "    \n",
    "    results[clf.clf_id].accuracy = accuracy \n",
    "    results[clf.clf_id].precision = precision\n",
    "    results[clf.clf_id].recall = recall\n",
    "    results[clf.clf_id].f1 = f1\n",
    "    results[clf.clf_id].kappa = kappa\n",
    "    \n",
    "\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "coated-thirty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc --- Accuracy: 0.498, Precision: 0.100, Recall: 0.200, F1: 0.133, Kappa: 0.000\n",
      "\n",
      "gnb --- Accuracy: 0.611, Precision: 0.521, Recall: 0.500, F1: 0.509, Kappa: 0.404\n",
      "\n",
      "lr --- Accuracy: 0.622, Precision: 0.553, Recall: 0.469, F1: 0.498, Kappa: 0.405\n",
      "\n",
      "lin_svm --- Accuracy: 0.609, Precision: 0.535, Recall: 0.425, F1: 0.450, Kappa: 0.375\n",
      "\n",
      "rbf_svm --- Accuracy: 0.627, Precision: 0.559, Recall: 0.463, F1: 0.493, Kappa: 0.413\n",
      "\n",
      "rf --- Accuracy: 0.627, Precision: 0.548, Recall: 0.492, F1: 0.514, Kappa: 0.422\n",
      "\n",
      "mlp --- Accuracy: 0.623, Precision: 0.546, Recall: 0.483, F1: 0.506, Kappa: 0.418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in json.loads(Encoder().encode(results)).items():\n",
    "    accuracy = v['accuracy']\n",
    "    precision = v['precision']\n",
    "    recall = v['recall']\n",
    "    f1 = v['f1']\n",
    "    kappa = v['kappa']\n",
    "    \n",
    "    print(f'{k} --- Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, Kappa: {kappa:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-fitness",
   "metadata": {},
   "source": [
    "# Best performing classifier on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consistent-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_val])\n",
    "y = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "systematic-malta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173594,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "functioning-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp/tmpbcx6_0_i'\", use \"location='/tmp/tmpbcx6_0_i'\" instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(FeatureUnion(transformer_list=[('text',\n",
      "                                Pipeline(steps=[('vectorizer',\n",
      "                                                 TfidfVectorizer(max_features=10000,\n",
      "                                                                 ngram_range=(1,\n",
      "                                                                              3))),\n",
      "                                                ('to_dense',\n",
      "                                                 <__main__.DenseTransformer object at 0x7f69b05c5590>),\n",
      "                                                ('lda',\n",
      "                                                 LinearDiscriminantAnalysis(n_components=4))])),\n",
      "                               ('feature_processing',\n",
      "                                Pipeline(steps=[('features',\n",
      "                                                 FeatureUnion(transformer_list=[('phrase_length',\n",
      "                                                                                 Pipeline(steps=[('f1',\n",
      "                                                                                                  FunctionTransformer(func=<function get_phrase_length at 0x7f69b08a9dd0>))]))]))]))]), \n",
      "147608    know truth man deconstructing format biography...\n",
      "143906                                    mostly worth trip\n",
      "90526          branched pseudowitty copycat interpretations\n",
      "41158                               Murder Numbers nt add .\n",
      "164045                                      Minority Report\n",
      "                                ...                        \n",
      "172980                                               paeans\n",
      "190797                                      dramatic scenes\n",
      "115760                                     blunt exposition\n",
      "235790                    without bludgeoning audience head\n",
      "108206                       engaged ferocious debate years\n",
      "Name: phrase_clean, Length: 173594, dtype: object, \n",
      "147608    3.0\n",
      "143906    3.0\n",
      "90526     2.0\n",
      "41158     2.0\n",
      "164045    3.0\n",
      "         ... \n",
      "172980    3.0\n",
      "190797    3.0\n",
      "115760    4.0\n",
      "235790    4.0\n",
      "108206    4.0\n",
      "Name: label_id, Length: 173594, dtype: float64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173594/173594 [00:00<00:00, 1876262.06it/s]\n",
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/pipeline.py:307: UserWarning: Persisting input arguments took 1.99s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 339.8s, 5.7min\n",
      "Making dense transformation...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1868587.57it/s]\n"
     ]
    }
   ],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "accuracy, precision, recall, f1, kappa = mlp.train_and_evaluate(X, y, \n",
    "                                                                X_test, y_test, \n",
    "                                                                use_features=True, \n",
    "                                                                memory=memory)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-starter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
