{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "described-class",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "diagnostic-oxygen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import Preprocessor\n",
    "from data_loader import DataLoader\n",
    "from helpers import select_n_components, pos_check\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-currency",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "variable-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3456/239073 [00:00<00:06, 34550.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239073/239073 [00:05<00:00, 41650.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = DataLoader().create_dataframe(preprocess=True, split=True, remove_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aerial-hours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>phrase</th>\n",
       "      <th>phrase_clean</th>\n",
       "      <th>sentiment_val</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29417</th>\n",
       "      <td>105996</td>\n",
       "      <td>Gives us a lot to chew on , but not all of it ...</td>\n",
       "      <td>Gives us lot chew properly digested .</td>\n",
       "      <td>0.41667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45769</th>\n",
       "      <td>108668</td>\n",
       "      <td>Rocky and Bullwinkle</td>\n",
       "      <td>Rocky Bullwinkle</td>\n",
       "      <td>0.51389</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129460</th>\n",
       "      <td>82606</td>\n",
       "      <td>if Nakata did it better</td>\n",
       "      <td>Nakata better</td>\n",
       "      <td>0.44444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18647</th>\n",
       "      <td>104291</td>\n",
       "      <td>An engrossing Iranian film about two itinerant...</td>\n",
       "      <td>An engrossing Iranian film two itinerant teach...</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9574</th>\n",
       "      <td>13622</td>\n",
       "      <td>, provocative , insistently humanizing</td>\n",
       "      <td>provocative insistently humanizing</td>\n",
       "      <td>0.68056</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        phrase_id                                             phrase  \\\n",
       "29417      105996  Gives us a lot to chew on , but not all of it ...   \n",
       "45769      108668                               Rocky and Bullwinkle   \n",
       "129460      82606                            if Nakata did it better   \n",
       "18647      104291  An engrossing Iranian film about two itinerant...   \n",
       "9574        13622             , provocative , insistently humanizing   \n",
       "\n",
       "                                             phrase_clean  sentiment_val  \\\n",
       "29417               Gives us lot chew properly digested .        0.41667   \n",
       "45769                                    Rocky Bullwinkle        0.51389   \n",
       "129460                                      Nakata better        0.44444   \n",
       "18647   An engrossing Iranian film two itinerant teach...        0.66667   \n",
       "9574                   provocative insistently humanizing        0.68056   \n",
       "\n",
       "        label_id     label  word_count  \n",
       "29417        3.0   Neutral           7  \n",
       "45769        3.0   Neutral           2  \n",
       "129460       3.0   Neutral           2  \n",
       "18647        4.0  Positive          19  \n",
       "9574         4.0  Positive           3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "anticipated-theorem",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126124, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-foundation",
   "metadata": {},
   "source": [
    "# Train, test, dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ignored-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['phrase_clean'], train['label_id']\n",
    "X_val, y_val = validate['phrase_clean'], validate['label_id']\n",
    "X_test, y_test = test['phrase_clean'], test['label_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-alarm",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection\n",
    "\n",
    "- Features to include:\n",
    "\n",
    "    - phrase length\n",
    "    - punctuation count\n",
    "    - capital letters count\n",
    "    - number of adjective POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "developed-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_count = lambda l1, l2: sum([1 for x in l1 if x in l2])\n",
    "caps_count = lambda l1: sum([1 for x in l1 if x.isupper()])\n",
    "\n",
    "def get_phrase_length(text):\n",
    "    return np.array([len(t) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_punct(text):\n",
    "    return np.array([punct_count(t, set(string.punctuation)) for t in tqdm(text)]).reshape(-1, 1)\n",
    "\n",
    "def get_num_caps(text):\n",
    "    return np.array([caps_count(t) for t in tqdm(text)]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "competent-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', \n",
    "                             max_features=10000, \n",
    "                             use_idf=True, \n",
    "                             ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-tucson",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "green-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationPipeline():\n",
    "    def __init__(self, memory, vectorizer, use_features):\n",
    "        \n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        print('Making dense transformation...')\n",
    "        return X.todense()\n",
    "\n",
    "\n",
    "def create_feature_pipeline(feature_name, feature_id, feature):\n",
    "    return (feature_name, Pipeline([\n",
    "        (feature_id, feature)\n",
    "    ]))\n",
    "\n",
    "def create_pipeline(my_id, clf, memory, vectorizer=tfidf_vect, use_features=True):\n",
    "    if use_features:\n",
    "        pipe = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4)),\n",
    "                ])),\n",
    "                create_feature_pipeline('phrase_length', 'f1', \n",
    "                                        FunctionTransformer(get_phrase_length, validate=False)),\n",
    "                create_feature_pipeline('num_punct', 'f2', \n",
    "                                        FunctionTransformer(get_num_punct, validate=False)),\n",
    "                create_feature_pipeline('num_caps', 'f3', \n",
    "                                        FunctionTransformer(get_num_caps, validate=False)),\n",
    "            ])),\n",
    "            (my_id, clf)            \n",
    "        ], memory=memory)\n",
    "    else:\n",
    "        pipe = Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('text', Pipeline([\n",
    "                    ('vectorizer', vectorizer),\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LinearDiscriminantAnalysis(n_components=4))\n",
    "                ])),\n",
    "            ])),\n",
    "            (my_id, clf)            \n",
    "        ])\n",
    "        \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-gibraltar",
   "metadata": {},
   "source": [
    "# Train classifiers with TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unnecessary-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "classifiers = {\n",
    "    'Dummy, most frequent': create_pipeline(my_id='dc', \n",
    "                                            clf=DummyClassifier(strategy='most_frequent'),\n",
    "                                            memory=memory,\n",
    "                                            use_features=True),\n",
    "    'Gaussian NB': create_pipeline(my_id='gnb', \n",
    "                                   clf=GaussianNB(), \n",
    "                                   memory=memory,\n",
    "                                   use_features=True),\n",
    "    'Logistic Regression': create_pipeline(my_id='lr', \n",
    "                                        clf=LogisticRegression(max_iter=8000), \n",
    "                                        memory=memory,\n",
    "                                        use_features=True),\n",
    "    'Linear SVM': create_pipeline(my_id='lin_svm', \n",
    "                                  clf=LinearSVC(), \n",
    "                                  memory=memory,\n",
    "                                  use_features=True),\n",
    "    'RBF SVM': create_pipeline(my_id='svm_rbf', \n",
    "                               clf=SVC(kernel='rbf'), \n",
    "                               memory=memory,\n",
    "                               use_features=True),\n",
    "    'Random Forest': create_pipeline(my_id='rf', \n",
    "                                     clf=RandomForestClassifier(max_depth=10, n_estimators=50),\n",
    "                                     memory=memory,\n",
    "                                     use_features=True),\n",
    "    'MLP Classifer': create_pipeline(my_id='mlp',\n",
    "                                     clf=MLPClassifier(max_iter=800),\n",
    "                                     memory=memory,\n",
    "                                     use_features=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-desperate",
   "metadata": {},
   "source": [
    "- Micro-averaged F1 score is preferable for scoring when dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "certain-boutique",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]2795.8s, 46.6min: Loading _fit_transform_one from /tmp/tmpyqku2wwm/joblib/sklearn/pipeline/_fit_transform_one/bdab5df99b107d978989bb6fa60bf478\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "\n",
      "Finished training classifier: Dummy, most frequent\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1882791.92it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 403909.58it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 505638.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1716514.02it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 332327.87it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 465620.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy, most frequent: 0.13290195526797016\n",
      "[Memory]2803.7s, 46.7min: Loading _fit_transform_one from /tmp/tmpyqku2wwm/joblib/sklearn/pipeline/_fit_transform_one/bdab5df99b107d978989bb6fa60bf478\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "\n",
      "Finished training classifier: Gaussian NB\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1860470.28it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 402348.06it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 500240.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1723154.51it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 331281.69it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 450995.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB: 0.4946757953164787\n",
      "[Memory]2811.7s, 46.9min: Loading _fit_transform_one from /tmp/tmpyqku2wwm/joblib/sklearn/pipeline/_fit_transform_one/bdab5df99b107d978989bb6fa60bf478\n",
      "___________________________________fit_transform_one cache loaded - 0.5s, 0.0min\n",
      "\n",
      "Finished training classifier: Logistic Regression\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1899756.51it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 406917.72it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 510775.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1672633.58it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 359795.46it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 442164.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.4984168813813622\n",
      "[Memory]2837.6s, 47.3min: Loading _fit_transform_one from /tmp/tmpyqku2wwm/joblib/sklearn/pipeline/_fit_transform_one/bdab5df99b107d978989bb6fa60bf478\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training classifier: Linear SVM\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1858476.75it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 401200.41it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 499352.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1708838.52it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 328767.02it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 460685.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM: 0.41898825408612295\n",
      "[Memory]2901.1s, 48.4min: Loading _fit_transform_one from /tmp/tmpyqku2wwm/joblib/sklearn/pipeline/_fit_transform_one/bdab5df99b107d978989bb6fa60bf478\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "\n",
      "Finished training classifier: RBF SVM\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1448190.86it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 401097.89it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 496437.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1700083.77it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 334083.84it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 468282.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVM: 0.49421423105242646\n",
      "[Memory]4309.6s, 71.8min: Loading _fit_transform_one from /tmp/tmpyqku2wwm/joblib/sklearn/pipeline/_fit_transform_one/bdab5df99b107d978989bb6fa60bf478\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "\n",
      "Finished training classifier: Random Forest\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1897405.69it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 389418.18it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 423113.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1725005.73it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 328533.75it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 458193.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.5111567105000308\n",
      "[Memory]4325.5s, 72.1min: Loading _fit_transform_one from /tmp/tmpyqku2wwm/joblib/sklearn/pipeline/_fit_transform_one/bdab5df99b107d978989bb6fa60bf478\n",
      "___________________________________fit_transform_one cache loaded - 0.6s, 0.0min\n",
      "\n",
      "Finished training classifier: MLP Classifer\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126124/126124 [00:00<00:00, 1879300.43it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 405830.74it/s]\n",
      "100%|██████████| 126124/126124 [00:00<00:00, 500264.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47470/47470 [00:00<00:00, 1423897.67it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 320778.38it/s]\n",
      "100%|██████████| 47470/47470 [00:00<00:00, 337606.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifer: 0.5155385014680038\n"
     ]
    }
   ],
   "source": [
    "ca_train_score = {}\n",
    "ca_val_score = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'\\nFinished training classifier: {name}')\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    \n",
    "    ca_train_score[name] = f1_score(train_preds, y_train, average='macro')\n",
    "    ca_val_score[name] = f1_score(val_preds, y_val, average='macro')\n",
    "    \n",
    "    print(f'{name}: {ca_val_score[name]}')\n",
    "    \n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "healthy-fields",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance on validation set: \n",
      "\n",
      "Validation (LHS), Training (RHS)\n",
      "\n",
      "Dummy, most frequent        0.133        0.136\n",
      "Gaussian NB                 0.495        0.566\n",
      "Logistic Regression         0.498        0.583\n",
      "Linear SVM                  0.419        0.495\n",
      "RBF SVM                     0.494        0.576\n",
      "Random Forest               0.511        0.625\n",
      "MLP Classifer               0.516        0.607\n"
     ]
    }
   ],
   "source": [
    "print('Classification performance on validation set: \\n')\n",
    "\n",
    "print('Validation (LHS), Training (RHS)')\n",
    "print()\n",
    "for name, clf in classifiers.items():\n",
    "    print (\"{method:<20s}{val_f1:>13.3f}{train_f1:>13.3f}\".format(\n",
    "        method=name, val_f1=ca_val_score[name],\n",
    "        train_f1=ca_train_score[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "amber-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose=10)\n",
    "\n",
    "classifiers = {\n",
    "    'Dummy, most frequent': create_pipeline(my_id='dc', \n",
    "                                            clf=DummyClassifier(strategy='most_frequent'),\n",
    "                                            memory=memory,\n",
    "                                            use_features=False),\n",
    "    'Gaussian NB': create_pipeline(my_id='gnb', \n",
    "                                   clf=GaussianNB(),\n",
    "                                   memory=memory,\n",
    "                                   use_features=False),\n",
    "    'Logistic Regression': create_pipeline(my_id='lr', \n",
    "                                        clf=LogisticRegression(max_iter=8000),\n",
    "                                        memory=memory,\n",
    "                                        use_features=False),\n",
    "    'Linear SVM': create_pipeline(my_id='lin_svm', \n",
    "                                  clf=LinearSVC(), \n",
    "                                  memory=memory,\n",
    "                                  use_features=False),\n",
    "    'RBF SVM': create_pipeline(my_id='svm_rbf', \n",
    "                               clf=SVC(kernel='rbf'), \n",
    "                               memory=memory,\n",
    "                               use_features=False),\n",
    "    'Random Forest': create_pipeline(my_id='rf', \n",
    "                                     clf=RandomForestClassifier(max_depth=10, n_estimators=50),\n",
    "                                     memory=memory,\n",
    "                                     use_features=False),\n",
    "    'MLP Classifer': create_pipeline(my_id='mlp',\n",
    "                                    clf=MLPClassifier(max_iter=800),\n",
    "                                     memory=memory,\n",
    "                                    use_features=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "finnish-sustainability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dense transformation...\n",
      "\n",
      "Finished training classifier: Dummy, most frequent\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "\n",
      "Finished training classifier: Gaussian NB\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "\n",
      "Finished training classifier: Logistic Regression\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s21/s2125219/miniconda3/envs/dme/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training classifier: Linear SVM\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "\n",
      "Finished training classifier: RBF SVM\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "\n",
      "Finished training classifier: Random Forest\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n",
      "\n",
      "Finished training classifier: MLP Classifer\n",
      "Making dense transformation...\n",
      "Making dense transformation...\n"
     ]
    }
   ],
   "source": [
    "ca_train_score = {}\n",
    "ca_val_score = {}\n",
    "\n",
    "ca_train_macro = {}\n",
    "ce_val_macro = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'\\nFinished training classifier: {name}')\n",
    "    \n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    \n",
    "    ca_train_score[name] = f1_score(train_preds, y_train, average='micro')\n",
    "    ca_val_score[name] = f1_score(val_preds, y_val, average='micro')\n",
    "    \n",
    "    ca_train_macro[name] = f1_score(train_preds, y_train, average='macro')\n",
    "    ce_val_macro[name] = f1_score(val_preds, y_val, average='macro')\n",
    "\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "israeli-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance on validation set: \n",
      "\n",
      "Validation (LHS), Training (RHS)\n",
      "\n",
      "Dummy, most frequent        0.498        0.133        0.518        0.136\n",
      "Gaussian NB                 0.607        0.504        0.678        0.585\n",
      "Logistic Regression         0.618        0.491        0.687        0.575\n",
      "Linear SVM                  0.611        0.466        0.683        0.555\n",
      "RBF SVM                     0.619        0.494        0.691        0.583\n",
      "Random Forest               0.620        0.505        0.709        0.617\n",
      "MLP Classifer               0.618        0.498        0.693        0.588\n"
     ]
    }
   ],
   "source": [
    "print('Classification performance on validation set: \\n')\n",
    "\n",
    "print('Validation (LHS), Training (RHS)')\n",
    "print()\n",
    "for name, clf in classifiers.items():\n",
    "    print (\"{method:<20s}{val_f1:>13.3f}{val_macro:>13.3f}{train_f1:>13.3f}{train_macro:>13.3f}\".format(\n",
    "        method=name, val_f1=ca_val_score[name], val_macro=ce_val_macro[name],\n",
    "        train_f1=ca_train_score[name], train_macro=ca_train_macro[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-nickel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
